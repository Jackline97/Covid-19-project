{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "unlabeled_final = pd.read_csv('data_unlabled_data_final_version2.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "confirmed_maclious_final = pd.read_csv('data_confirmed_CovidURL_final_version2.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "white_list_final = pd.read_csv('data_Confirmed_whitelist_final_version2.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "Bing_final = pd.read_csv('Bing_final_version.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "phish_tank_final = pd.read_csv('phish_tank_final_version.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unified_url</th>\n",
       "      <th>Reachable_URL</th>\n",
       "      <th>Way_back_archived</th>\n",
       "      <th>Freenom_top_level_domain</th>\n",
       "      <th>Previous_malicious_top_level_domain_TLD</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Wrong_spell_List</th>\n",
       "      <th>Longest_word_ratio</th>\n",
       "      <th>Special_mark</th>\n",
       "      <th>sub_domain</th>\n",
       "      <th>Contain_Weried_number_combination</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>Alexa_rank</th>\n",
       "      <th>Status_code</th>\n",
       "      <th>start_date</th>\n",
       "      <th>wildcard_subdomain</th>\n",
       "      <th>Redirect_URL</th>\n",
       "      <th>Created on 2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://coronavirusemploymentservices.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://coronavirusen.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>503</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://coronavirusencasa.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>503</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://coronavirusencolombia.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>404</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://coronavirusend.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Unified_url  Reachable_URL  Way_back_archived  \\\n",
       "0  http://coronavirusemploymentservices.com              1                  0   \n",
       "1                  http://coronavirusen.com              0                  0   \n",
       "2              http://coronavirusencasa.com              0                  0   \n",
       "3          http://coronavirusencolombia.com              0                  0   \n",
       "4                 http://coronavirusend.com              1                  0   \n",
       "\n",
       "   Freenom_top_level_domain  Previous_malicious_top_level_domain_TLD  \\\n",
       "0                         0                                        0   \n",
       "1                         0                                        0   \n",
       "2                         0                                        0   \n",
       "3                         0                                        0   \n",
       "4                         0                                        0   \n",
       "\n",
       "   Name_length  Wrong_spell_List  Longest_word_ratio  Special_mark  \\\n",
       "0            6                 0            0.050000             0   \n",
       "1            5                 0            0.083333             0   \n",
       "2            6                 0            0.071429             0   \n",
       "3            6                 0            0.062500             0   \n",
       "4            5                 0            0.080000             0   \n",
       "\n",
       "   sub_domain  Contain_Weried_number_combination  levenshtein_distance  \\\n",
       "0           2                                  0                    14   \n",
       "1           2                                  0                     3   \n",
       "2           2                                  0                     5   \n",
       "3           2                                  0                     8   \n",
       "4           2                                  0                     3   \n",
       "\n",
       "   Alexa_rank  Status_code  start_date  wildcard_subdomain  Redirect_URL  \\\n",
       "0           0          200        2018                   0             0   \n",
       "1           0          503           0                   0             0   \n",
       "2           0          503           0                   0             0   \n",
       "3           1          404        2009                   0             0   \n",
       "4           0          200        2018                   1             0   \n",
       "\n",
       "   Created on 2020  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_unlabeled = unlabeled_final['Unified_url']\n",
    "url_phishtank = phish_tank_final['Unified_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229489/229489 [00:00<00:00, 405352.94it/s]\n"
     ]
    }
   ],
   "source": [
    "url_unlabeled_dom = []\n",
    "getattr(tqdm, '_instances', {}).clear()\n",
    "for i in tqdm(url_unlabeled):\n",
    "    hostname = i\n",
    "    pattern = \"https://|http://|www.|https://www.|http://www.\"\n",
    "    pre_pattern_match = re.search(pattern, hostname)\n",
    "    if pre_pattern_match:\n",
    "        hostname = hostname[pre_pattern_match.end():]\n",
    "        post_pattern_match = re.search(\"/\", hostname)\n",
    "        if post_pattern_match:\n",
    "            hostname = hostname[:post_pattern_match.start()]\n",
    "        char1 = hostname\n",
    "        url_unlabeled_dom.append(char1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98251/98251 [00:00<00:00, 393345.74it/s]\n"
     ]
    }
   ],
   "source": [
    "url_phishtank_dom = []\n",
    "getattr(tqdm, '_instances', {}).clear()\n",
    "for i in tqdm(url_phishtank):\n",
    "    hostname = i\n",
    "    pattern = \"https://|http://|www.|https://www.|http://www.\"\n",
    "    pre_pattern_match = re.search(pattern, hostname)\n",
    "    if pre_pattern_match:\n",
    "        hostname = hostname[pre_pattern_match.end():]\n",
    "        post_pattern_match = re.search(\"/\", hostname)\n",
    "        if post_pattern_match:\n",
    "            hostname = hostname[:post_pattern_match.start()]\n",
    "        char1 = hostname\n",
    "        url_phishtank_dom.append(char1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229489/229489 [18:44<00:00, 204.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "common_list = []\n",
    "for i in tqdm(url_unlabeled_dom):\n",
    "    for j in url_phishtank_dom:\n",
    "        if i == j:\n",
    "             common_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98251/98251 [00:00<00:00, 1281464.27it/s]\n"
     ]
    }
   ],
   "source": [
    "phish_list = []\n",
    "key_word = ['covid','virus','corna','c0vid','cov1d']\n",
    "for i in tqdm(url_phishtank_dom):\n",
    "    for j in key_word:\n",
    "        if j in i:\n",
    "            phish_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "phish_list_phishtank = np.unique(phish_list + common_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [00:58<00:00,  2.71it/s]\n",
      "100%|██████████| 158/158 [01:40<00:00,  1.57it/s]\n",
      "100%|██████████| 158/158 [05:36<00:00,  2.13s/it]\n",
      "  1%|▏         | 2/158 [00:00<00:13, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The produced word ratio list's length is  158\n",
      "The sample data is [0.125, 0.05555555555555555, 0.10526315789473684, 0.02531645569620253, 0.09523809523809523, 0.08695652173913043]\n",
      "The number of domains cotain symbol '-': 86\n",
      "The number of domain contain IP adress is 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [00:12<00:00, 12.93it/s]\n",
      "100%|██████████| 158/158 [00:59<00:00,  2.64it/s]\n",
      "100%|██████████| 158/158 [02:27<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import wordninja\n",
    "import http\n",
    "import urllib.request, urllib.error\n",
    "from math import log\n",
    "from socket import timeout\n",
    "def Feature_extraction(List_all):\n",
    "\n",
    "    def Extract_FTLD(List_all):\n",
    "        Freenom_top_level_domain = []\n",
    "        for i in List_all:\n",
    "            if i[-2:] == 'ml':\n",
    "                Freenom_top_level_domain.append(1)\n",
    "            elif i[-2:] == 'cf':\n",
    "                Freenom_top_level_domain.append(1)\n",
    "            elif i[-2:] == 'gq':\n",
    "                Freenom_top_level_domain.append(1)\n",
    "            elif i[-2:] == 'tk':\n",
    "                Freenom_top_level_domain.append(1)\n",
    "            elif i[-2:] == 'ga':\n",
    "                Freenom_top_level_domain.append(1)\n",
    "            else:\n",
    "                Freenom_top_level_domain.append(0)\n",
    "        return Freenom_top_level_domain\n",
    "\n",
    "    def Previous_MTLD(List_all):\n",
    "    # According to https://blogs.akamai.com/2019/10/a-view-into-top-level-domain-tld-abuse.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+TheAkamaiBlog+%28The+Akamai+Blog%29\n",
    "    # According to https://www.anomali.com/blog/abusing-the-mali-cctld-ml-to-target-dutch-organisations\n",
    "        Previous_malicious_top_level_domain_TLD = []\n",
    "        for i in List_all:\n",
    "            if i[-2:] == 'ml':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'cf':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'so':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-4:] == 'loan':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-5:] == 'tokyo':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-5:] == 'trade':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-6:] == 'stream':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-3:] == 'bid':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-3:] == 'icu':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-3:] == 'gdn':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-3:] == 'win':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-4:] == 'work':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-4:] == 'desi':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-4:] == 'pics':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'gq':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'tk':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'vg':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'ga':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'to':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'cc':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'hk':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'pw':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'fm':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            elif i[-2:] == 'la':\n",
    "                Previous_malicious_top_level_domain_TLD.append(1)\n",
    "            else:\n",
    "                Previous_malicious_top_level_domain_TLD.append(0)\n",
    "        return Previous_malicious_top_level_domain_TLD\n",
    "\n",
    "    def mixed_feature(List_all):\n",
    "        # Build a cost dictionary, assuming Zipf's law and cost = -math.log(probability).\n",
    "        # words = open(\"words-by-frequency.txt\").read().split()\n",
    "        # wordcost = dict((k, log((i+1)*log(len(words)))) for i,k in enumerate(words))\n",
    "        # maxword = max(len(x) for x in words)\n",
    "\n",
    "        def infer_spaces(s):\n",
    "            \"\"\"Uses dynamic programming to infer the location of spaces in a string\n",
    "            without spaces.\"\"\"\n",
    "\n",
    "            # Find the best match for the i first characters, assuming cost has\n",
    "            # been built for the i-1 first characters.\n",
    "            # Returns a pair (match_cost, match_length).\n",
    "            def best_match(i):\n",
    "                candidates = enumerate(reversed(cost[max(0, i-maxword):i]))\n",
    "                return min((c + wordcost.get(s[i-k-1:i], 9e999), k+1) for k,c in candidates)\n",
    "\n",
    "            # Build the cost array.\n",
    "            cost = [0]\n",
    "            for i in range(1,len(s)+1):\n",
    "                c,k = best_match(i)\n",
    "                cost.append(c)\n",
    "\n",
    "            # Backtrack to recover the minimal-cost string.\n",
    "            out = []\n",
    "            i = len(s)\n",
    "            while i>0:\n",
    "                c,k = best_match(i)\n",
    "                assert c == cost[i]\n",
    "                out.append(s[i-k:i])\n",
    "                i -= k\n",
    "\n",
    "            return \" \".join(reversed(out))\n",
    "\n",
    "        # This would take some time\n",
    "        word_dic = []\n",
    "        number_mark = []\n",
    "        Name_length = []\n",
    "        Wrong_spell = [\"cov1d\",\"c0v1d\",\"c0vid\",\"c0rona\",\"c0r0na\",\"cor0na\",\"v1rus\",\"coivd\",'co1vd']\n",
    "        Wrong_spell = Wrong_spell \n",
    "        Wrong_spell_List = []\n",
    "\n",
    "        for i in List_all:\n",
    "        #     reduce the prefix and sufix\n",
    "            temp_mark = 0\n",
    "            for j in Wrong_spell:\n",
    "                if j in i:\n",
    "                    temp_mark += 1          \n",
    "            if temp_mark>0:\n",
    "                Wrong_spell_List.append(1)\n",
    "            else:\n",
    "                Wrong_spell_List.append(0)\n",
    "            original_len = len(i) - 2 \n",
    "            Each_List = wordninja.split(i)\n",
    "            Name_length.append(len(Each_List))\n",
    "            stand = 0\n",
    "            for j in Each_List:\n",
    "                if str.isdigit(j) is True:\n",
    "                    number_mark.append(1)\n",
    "                    stand = 1\n",
    "                break\n",
    "            if stand==0:\n",
    "                number_mark.append(0)\n",
    "            longest_element = max([(len(x),x) for x in Each_List])\n",
    "            Ratio = len(longest_element)/original_len\n",
    "            word_dic.append(Ratio)\n",
    "        print(\"The produced word ratio list's length is \",len(word_dic))\n",
    "        print(\"The sample data is\",word_dic[0:6])\n",
    "        return Name_length, Wrong_spell_List,word_dic\n",
    "\n",
    "    def Find_Dash_mark(List_all):\n",
    "        Special_mark = []\n",
    "        for i in List_all:\n",
    "            if '-' in i:\n",
    "        #         print(i)\n",
    "                Special_mark.append(1)\n",
    "            else:\n",
    "                Special_mark.append(0)\n",
    "        print(\"The number of domains cotain symbol '-':\",Special_mark.count(1))\n",
    "        return Special_mark\n",
    "\n",
    "    def Count_subdomain(List_all):\n",
    "        sub_domain = []\n",
    "        for i in List_all:\n",
    "            dot_num = i.count('.') \n",
    "            sub_domain.append(dot_num)\n",
    "        return sub_domain\n",
    "\n",
    "    def Contain_IP_address(List_all):\n",
    "        import re\n",
    "        Contain_IP_Adress = []\n",
    "        for i in List_all:\n",
    "            IP = re.findall(r\".\\d.\\d.\\d\",i)\n",
    "            if len(IP) == 0:\n",
    "                Contain_IP_Adress.append(0)\n",
    "            else:\n",
    "                Contain_IP_Adress.append(1)\n",
    "        print(\"The number of domain contain IP adress is\",Contain_IP_Adress.count(1))\n",
    "        return Contain_IP_Adress \n",
    "    import requests\n",
    "    from tqdm import tqdm\n",
    "    from bs4 import BeautifulSoup\n",
    "    import json\n",
    "    Alexa_rank_phish_w = []\n",
    "    Status_phish_w = []\n",
    "    getattr(tqdm, '_instances', {}).clear()\n",
    "    for i in tqdm(List_all):\n",
    "        url1 = 'https://awis.api.alexa.com/api?Action=UrlInfo&Count=10&ResponseGroup=Rank,LinksInCount&Start=1&Url='\n",
    "        url = url1+i\n",
    "        headers={'x-api-key':'R90lWSm4iC6L6zDUZnZgs8UmqmtUCFrB6fCT2EY5'}\n",
    "        response = requests.get(url,headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        try:\n",
    "            statuscode = soup.findAll(\"statuscode\")[0].string\n",
    "            if statuscode is None:\n",
    "                Status_phish_w.append(0)\n",
    "            else:\n",
    "                Status_phish_w.append(statuscode)\n",
    "        except (IndexError,ConnectionError):\n",
    "                Status_phish_w.append(0)\n",
    "                print(\"sa\")\n",
    "                print(i)\n",
    "        try:\n",
    "            rank = soup.findAll(\"rank\")[0].string\n",
    "            if rank is None:\n",
    "                Alexa_rank_phish_w.append(0)\n",
    "            else:\n",
    "                Alexa_rank_phish_w.append(rank)\n",
    "        except (IndexError,ConnectionError):\n",
    "                Alexa_rank_phish_w.append(0)\n",
    "                print(\"wa\")\n",
    "                print(i)\n",
    "    way_back_phish_w = []\n",
    "    time_stamp_phish_w = []\n",
    "    reachable_url_phish_w = []\n",
    "    \n",
    "    getattr(tqdm, '_instances', {}).clear()\n",
    "    for i in tqdm(List_all):\n",
    "        try:\n",
    "            url1 = 'https://archive.org/wayback/available?url='\n",
    "            url = url1 + i\n",
    "            response = requests.get(url,timeout=1)\n",
    "            json_data = json.loads(response.text)\n",
    "            status= response.status_code\n",
    "            if status == 200:\n",
    "                reachable_url_phish_w.append(1)\n",
    "            else:\n",
    "                reachable_url_phish_w.append(0)\n",
    "            if bool(json_data['archived_snapshots']) is False:\n",
    "                way_back_phish_w.append(0)\n",
    "                time_stamp_phish_w.append(0)\n",
    "            else:\n",
    "                way_back_phish_w.append(1)\n",
    "                time_stamp_phish_w.append(json_data['archived_snapshots']['closest']['timestamp'])\n",
    "\n",
    "        except requests.Timeout as err:\n",
    "            way_back_phish_w.append(0)\n",
    "            time_stamp_phish_w.append(0)\n",
    "            reachable_url_phish_w.append(0)\n",
    "    check_404_phishtank2 = []\n",
    "    getattr(tqdm, '_instances', {}).clear()\n",
    "    for i in tqdm(List_all):\n",
    "        url = 'https://'+i\n",
    "        try:\n",
    "            conn = urllib.request.urlopen(url,timeout = 10)\n",
    "        except urllib.error.HTTPError as e:\n",
    "            check_404_phishtank2.append(e.code)\n",
    "        except urllib.error.URLError as e:\n",
    "            # Not an HTTP-specific error (e.g. connection refused)\n",
    "            # ...\n",
    "            check_404_phishtank2.append(e.reason)\n",
    "        except ConnectionError as e:\n",
    "            check_404_phishtank2.append(e)\n",
    "        except timeout:\n",
    "             check_404_phishtank2.append('time out')\n",
    "        except http.client.BadStatusLine as e:\n",
    "            check_404_phishtank2.append('unknown structure')\n",
    "        except http.client.InvalidURL as e:\n",
    "            check_404_phishtank2.append('url error')\n",
    "        else:\n",
    "            # 200\n",
    "            # ...\n",
    "            check_404_phishtank2.append(200)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Freenom_top_level_domain = Extract_FTLD(List_all)\n",
    "    Previous_malicious_top_level_domain_TLD = Previous_MTLD(List_all)\n",
    "    Name_length, Wrong_spell_List, word_dic = mixed_feature(List_all)\n",
    "    Special_mark = Find_Dash_mark(List_all)\n",
    "    sub_domain = Count_subdomain(List_all)\n",
    "    Contain_IP_Adress = Contain_IP_address(List_all)\n",
    "    return Freenom_top_level_domain,Previous_malicious_top_level_domain_TLD,Name_length, Wrong_spell_List,word_dic,Special_mark,sub_domain,Contain_IP_Adress,Alexa_rank_phish_w,way_back_phish_w,check_404_phishtank2\n",
    "\n",
    "Freenom_top_level_domain,Previous_malicious_top_level_domain_TLD,Name_length, Wrong_spell_List,word_dic,Special_mark,sub_domain,Contain_IP_Adress,Alexa_rank_phish_w,way_back_phish_w,check_404_phishtank2 = Feature_extraction(phish_list_phishtank)\n",
    "Status_code = []\n",
    "for i in check_404_phishtank2:\n",
    "    i = str(i)\n",
    "#     print(i[:3])\n",
    "    if i[0] == '[':\n",
    "        Status_code.append(500)\n",
    "    elif i[0] == 't':\n",
    "        Status_code.append(598)\n",
    "    else:\n",
    "        i = int(i)\n",
    "        Status_code.append(i)\n",
    "def extract_feature_from_status(templist):\n",
    "    blocked_list = []\n",
    "    redirected_list = []\n",
    "    Reachable_list = []\n",
    "    for i in templist:\n",
    "        i = int(i)\n",
    "        if i < 300:\n",
    "            Reachable_list.append(1)\n",
    "        else:\n",
    "            Reachable_list.append(0)\n",
    "    for i in templist:\n",
    "        i = int(i)\n",
    "        if i > 200 and i < 400:\n",
    "            redirected_list.append(1)\n",
    "        else:\n",
    "            redirected_list.append(0)\n",
    "    for i in templist:\n",
    "        i = int(i)\n",
    "        if i > 400:\n",
    "            blocked_list.append(1)\n",
    "        else:\n",
    "            blocked_list.append(0)\n",
    "    return Reachable_list,redirected_list\n",
    "\n",
    "Reachable_list_unlabeled, redirected_list_unlabeled =  extract_feature_from_status(Status_code)\n",
    "\n",
    "import numpy as np\n",
    "confirmed_phishing_website = []\n",
    "f = open(\"ConfirmedPhishing.txt\", \"r\")\n",
    "for x in f:\n",
    "    confirmed_phishing_website.append('http://'+ x[3:-4])\n",
    "    \n",
    "def get_hostname_from_url(url):\n",
    "    hostname = url\n",
    "    # TODO: Put this pattern in patterns.py as something like - get_hostname_pattern.\n",
    "    pattern = \"https://|http://|www.|https://www.|http://www.\"\n",
    "    pre_pattern_match = re.search(pattern, hostname)\n",
    "\n",
    "    if pre_pattern_match:\n",
    "        hostname = hostname[pre_pattern_match.end():]\n",
    "        post_pattern_match = re.search(\"/\", hostname)\n",
    "        if post_pattern_match:\n",
    "            hostname = hostname[:post_pattern_match.start()]\n",
    "\n",
    "    return hostname\n",
    "\n",
    "def levenshtein(source,target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "def cal_min_lev(source, target):\n",
    "    min_dis_list = []\n",
    "    for i in tqdm(source):\n",
    "        i  = get_hostname_from_url(i)\n",
    "        temp_list = []\n",
    "        for j in target:\n",
    "            j = get_hostname_from_url(j)\n",
    "            temp_list.append(levenshtein(i,j))\n",
    "        min_dis_list.append(min(temp_list))\n",
    "    return min_dis_list\n",
    "\n",
    "lev_dis = cal_min_lev(phish_list_phishtank, confirmed_phishing_website)\n",
    "import subprocess\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "def dig_wildCard_domain(url_list):\n",
    "    wildcard_subdomain = []\n",
    "    getattr(tqdm, '_instances', {}).clear()\n",
    "    for url in tqdm(url_list):\n",
    "        char = url\n",
    "        if char[:3] == 'www':\n",
    "            position = [m.start() for m in re.finditer('[.]',url)]\n",
    "            char = url[position[0]:][1:]\n",
    "            search_char = '*' + '.' + char\n",
    "#             print(search_char)\n",
    "            out = subprocess.Popen(['dig', '+short', search_char], \n",
    "                   stdout=subprocess.PIPE, \n",
    "                   stderr=subprocess.STDOUT)\n",
    "            stdout,stderr = out.communicate()\n",
    "            if len(stdout)!= 0:\n",
    "                wildcard_subdomain.append(1)\n",
    "            else:\n",
    "                wildcard_subdomain.append(0)\n",
    "        else:   \n",
    "            search_char = '*' + '.' + char\n",
    "#             print(search_char)\n",
    "            out = subprocess.Popen(['dig', '+short', search_char], \n",
    "                   stdout=subprocess.PIPE, \n",
    "                   stderr=subprocess.STDOUT)\n",
    "            stdout,stderr = out.communicate()\n",
    "#             print(stdout)\n",
    "            if len(stdout)!= 0:\n",
    "                wildcard_subdomain.append(1)\n",
    "            else:\n",
    "                wildcard_subdomain.append(0)\n",
    "    return wildcard_subdomain\n",
    "wildcard_subdomain_Bing = dig_wildCard_domain(phish_list_phishtank)\n",
    "import whois\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "Init_date_list21 = []\n",
    "getattr(tqdm, '_instances', {}).clear()\n",
    "for i in tqdm(phish_list_phishtank):\n",
    "    try:\n",
    "        hostname = i\n",
    "        char = hostname\n",
    "        if char[-6:] == 'online':\n",
    "            out = subprocess.Popen(['whois',  char], \n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.STDOUT)\n",
    "            stdout,stderr = out.communicate()\n",
    "            stdout = stdout.decode('utf-8')\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "        elif char[-2:] == 'ws':\n",
    "            out = subprocess.Popen(['whois',  char], \n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.STDOUT)\n",
    "            stdout,stderr = out.communicate()\n",
    "            stdout = stdout.decode('utf-8')\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))        \n",
    "        else:\n",
    "            domain = whois.query(char)\n",
    "            init_date = domain.creation_date\n",
    "            Init_date_list21.append(init_date.year)\n",
    "    except whois.UnknownTld:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        try:\n",
    "            stdout = stdout.decode('utf-8')\n",
    "            try:\n",
    "                created_date1 = re.search('created:',stdout).end()\n",
    "                created_date2 = re.search('changed:',stdout).start()\n",
    "                Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "            except AttributeError:\n",
    "                Init_date_list21.append(0)\n",
    "        except UnicodeDecodeError:\n",
    "            stdout = stdout.decode('ISO-8859-1')\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "    except AttributeError as e:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        try:\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "        except AttributeError as e:\n",
    "            Init_date_list21.append(0)\n",
    "    except KeyError:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        created_date1 = re.search('created:',stdout).end()\n",
    "        created_date2 = re.search('changed:',stdout).start()\n",
    "        Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "    except whois.UnknownDateFormat:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        try:\n",
    "            stdout = stdout.decode('utf-8')\n",
    "            try:\n",
    "                created_date1 = re.search('created:',stdout).end()\n",
    "                created_date2 = re.search('changed:',stdout).start()\n",
    "                Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "            except AttributeError as e:\n",
    "                Init_date_list21.append(2003)\n",
    "        except UnicodeDecodeError:\n",
    "            out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "            stdout,stderr = out.communicate()\n",
    "            stdout = stdout.decode('ISO-8859-1')\n",
    "            try:\n",
    "                created_date1 = re.search('created:',stdout).end()\n",
    "                created_date2 = re.search('changed:',stdout).start()\n",
    "                Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "            except AttributeError:\n",
    "                Init_date_list21.append(0)\n",
    "            \n",
    "            \n",
    "    except whois.WhoisCommandFailed:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        try:\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "        except AttributeError:\n",
    "            Init_date_list21.append(0)\n",
    "    except UnicodeDecodeError:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('ISO-8859-1')\n",
    "        try:\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "        except AttributeError:\n",
    "            Init_date_list21.append(0)\n",
    "    except whois.FailedParsingWhoisOutput as e:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('ISO-8859-1')\n",
    "        created_date1 = re.search('created:',stdout).end()\n",
    "        created_date2 = re.search('changed:',stdout).start()\n",
    "        Init_date_list21.append(int(stdout[created_date1+6:created_date2][:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_check = []\n",
    "for i in Init_date_list21:\n",
    "    if i == 2020:\n",
    "        temp_check.append(1)\n",
    "    else:\n",
    "        temp_check.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_phish_Mal = {'Unified_url':phish_list_phishtank,'Reachable_URL':Reachable_list_unlabeled,\n",
    "                  'Way_back_archived':way_back_phish_w,\n",
    "                   \"Freenom_top_level_domain\":Freenom_top_level_domain,\n",
    "                   \"Previous_malicious_top_level_domain_TLD\":Previous_malicious_top_level_domain_TLD,\n",
    "                   \"Name_length\":Name_length,\"Wrong_spell_List\":Wrong_spell_List,\"Longest_word_ratio\":word_dic,\n",
    "                   \"Special_mark\":Special_mark,\"sub_domain\":sub_domain,\n",
    "                   'Contain_Weried_number_combination':Contain_IP_Adress,\n",
    "                   'levenshtein_distance':lev_dis,'Alexa_rank':Alexa_rank_phish_w,'Status_code':Status_code,\"start_date\":Init_date_list21,\n",
    "                   \"wildcard_subdomain\":wildcard_subdomain_Bing,\n",
    "                  'Redirect_URL':redirected_list_unlabeled,\n",
    "                  \n",
    "                  'Created on 2020':temp_check}\n",
    "sample_data_Mal = pd.DataFrame(dict_phish_Mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394 entries, 0 to 393\n",
      "Data columns (total 18 columns):\n",
      "Unified_url                                394 non-null object\n",
      "Reachable_URL                              394 non-null int64\n",
      "Way_back_archived                          394 non-null int64\n",
      "Freenom_top_level_domain                   394 non-null int64\n",
      "Previous_malicious_top_level_domain_TLD    394 non-null int64\n",
      "Name_length                                394 non-null int64\n",
      "Wrong_spell_List                           394 non-null int64\n",
      "Longest_word_ratio                         394 non-null float64\n",
      "Special_mark                               394 non-null int64\n",
      "sub_domain                                 394 non-null int64\n",
      "Contain_Weried_number_combination          394 non-null int64\n",
      "levenshtein_distance                       394 non-null int64\n",
      "Alexa_rank                                 394 non-null int64\n",
      "Status_code                                394 non-null int64\n",
      "start_date                                 394 non-null int64\n",
      "wildcard_subdomain                         394 non-null int64\n",
      "Redirect_URL                               394 non-null int64\n",
      "Created on 2020                            394 non-null int64\n",
      "dtypes: float64(1), int64(16), object(1)\n",
      "memory usage: 55.5+ KB\n"
     ]
    }
   ],
   "source": [
    "confirmed_maclious_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 158 entries, 0 to 157\n",
      "Data columns (total 18 columns):\n",
      "Unified_url                                158 non-null object\n",
      "Reachable_URL                              158 non-null int64\n",
      "Way_back_archived                          158 non-null int64\n",
      "Freenom_top_level_domain                   158 non-null int64\n",
      "Previous_malicious_top_level_domain_TLD    158 non-null int64\n",
      "Name_length                                158 non-null int64\n",
      "Wrong_spell_List                           158 non-null int64\n",
      "Longest_word_ratio                         158 non-null float64\n",
      "Special_mark                               158 non-null int64\n",
      "sub_domain                                 158 non-null int64\n",
      "Contain_Weried_number_combination          158 non-null int64\n",
      "levenshtein_distance                       158 non-null int64\n",
      "Alexa_rank                                 158 non-null object\n",
      "Status_code                                158 non-null int64\n",
      "start_date                                 158 non-null int64\n",
      "wildcard_subdomain                         158 non-null int64\n",
      "Redirect_URL                               158 non-null int64\n",
      "Created on 2020                            158 non-null int64\n",
      "dtypes: float64(1), int64(15), object(2)\n",
      "memory usage: 22.3+ KB\n"
     ]
    }
   ],
   "source": [
    "sample_data_Mal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mal_final = pd.concat( [confirmed_maclious_final, sample_data_Mal], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 552 entries, 0 to 157\n",
      "Data columns (total 18 columns):\n",
      "Unified_url                                552 non-null object\n",
      "Reachable_URL                              552 non-null int64\n",
      "Way_back_archived                          552 non-null int64\n",
      "Freenom_top_level_domain                   552 non-null int64\n",
      "Previous_malicious_top_level_domain_TLD    552 non-null int64\n",
      "Name_length                                552 non-null int64\n",
      "Wrong_spell_List                           552 non-null int64\n",
      "Longest_word_ratio                         552 non-null float64\n",
      "Special_mark                               552 non-null int64\n",
      "sub_domain                                 552 non-null int64\n",
      "Contain_Weried_number_combination          552 non-null int64\n",
      "levenshtein_distance                       552 non-null int64\n",
      "Alexa_rank                                 552 non-null object\n",
      "Status_code                                552 non-null int64\n",
      "start_date                                 552 non-null int64\n",
      "wildcard_subdomain                         552 non-null int64\n",
      "Redirect_URL                               552 non-null int64\n",
      "Created on 2020                            552 non-null int64\n",
      "dtypes: float64(1), int64(15), object(2)\n",
      "memory usage: 81.9+ KB\n"
     ]
    }
   ],
   "source": [
    "Mal_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "temp = 0 \n",
    "for i in Mal_final['start_date']:\n",
    "    if i == 0:\n",
    "        temp+=1\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 46/552 [04:00<44:00,  5.22s/it]\n",
      "100%|██████████| 552/552 [08:40<00:00,  1.06it/s]  \n"
     ]
    }
   ],
   "source": [
    "import whois\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "Init_date_list17 = []\n",
    "getattr(tqdm, '_instances', {}).clear()\n",
    "for i in tqdm(Mal_final['Unified_url']):\n",
    "    try:\n",
    "        hostname = i\n",
    "        pattern = \"https://|http://|www.|https://www.|http://www.\"\n",
    "        pre_pattern_match = re.search(pattern, hostname)\n",
    "        if pre_pattern_match:\n",
    "            hostname = hostname[pre_pattern_match.end():]\n",
    "            post_pattern_match = re.search(\"/\", hostname)\n",
    "            if post_pattern_match:\n",
    "                hostname = hostname[:post_pattern_match.start()]\n",
    "            char = hostname\n",
    "        if char[-6:] == 'online':\n",
    "            out = subprocess.Popen(['whois',  char], \n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.STDOUT)\n",
    "            stdout,stderr = out.communicate()\n",
    "            stdout = stdout.decode('utf-8')\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "        elif char[-2:] == 'ws':\n",
    "            out = subprocess.Popen(['whois',  char], \n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.STDOUT)\n",
    "            stdout,stderr = out.communicate()\n",
    "            stdout = stdout.decode('utf-8')\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))        \n",
    "        else:\n",
    "            domain = whois.query(char)\n",
    "            init_date = domain.creation_date\n",
    "            Init_date_list17.append(init_date.year)\n",
    "    except whois.UnknownTld:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        try:\n",
    "            stdout = stdout.decode('utf-8')\n",
    "            try:\n",
    "                created_date1 = re.search('created:',stdout).end()\n",
    "                created_date2 = re.search('changed:',stdout).start()\n",
    "                Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "            except AttributeError:\n",
    "                Init_date_list17.append(0)\n",
    "        except UnicodeDecodeError:\n",
    "            stdout = stdout.decode('ISO-8859-1')\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "    except AttributeError as e:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        try:\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "        except AttributeError as e:\n",
    "            Init_date_list17.append(0)\n",
    "    except KeyError:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        created_date1 = re.search('created:',stdout).end()\n",
    "        created_date2 = re.search('changed:',stdout).start()\n",
    "        Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "    except whois.UnknownDateFormat:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        try:\n",
    "            stdout = stdout.decode('utf-8')\n",
    "            try:\n",
    "                created_date1 = re.search('created:',stdout).end()\n",
    "                created_date2 = re.search('changed:',stdout).start()\n",
    "                Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "            except AttributeError as e:\n",
    "                Init_date_list17.append(2003)\n",
    "        except UnicodeDecodeError:\n",
    "            out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "            stdout,stderr = out.communicate()\n",
    "            stdout = stdout.decode('ISO-8859-1')\n",
    "            try:\n",
    "                created_date1 = re.search('created:',stdout).end()\n",
    "                created_date2 = re.search('changed:',stdout).start()\n",
    "                Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "            except AttributeError:\n",
    "                Init_date_list17.append(0)\n",
    "            \n",
    "            \n",
    "    except whois.WhoisCommandFailed:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('utf-8')\n",
    "        try:\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "        except AttributeError:\n",
    "            Init_date_list17.append(0)\n",
    "    except UnicodeDecodeError:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('ISO-8859-1')\n",
    "        try:\n",
    "            created_date1 = re.search('created:',stdout).end()\n",
    "            created_date2 = re.search('changed:',stdout).start()\n",
    "            Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))\n",
    "        except AttributeError:\n",
    "            Init_date_list17.append(0)\n",
    "    except whois.FailedParsingWhoisOutput as e:\n",
    "        out = subprocess.Popen(['whois',  char],stdout=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "        stdout,stderr = out.communicate()\n",
    "        stdout = stdout.decode('ISO-8859-1')\n",
    "        created_date1 = re.search('created:',stdout).end()\n",
    "        created_date2 = re.search('changed:',stdout).start()\n",
    "        Init_date_list17.append(int(stdout[created_date1+6:created_date2][:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mal_final['start_date'] = Init_date_list17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mal_final = Mal_final.drop(Mal_final[Mal_final[\"start_date\"]==0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mal_final.to_csv('data_confirmed_CovidURL_final_version3.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackline/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "white_list_final = pd.concat( [white_list_final, Bing_final], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_phish_white = {'Unified_url':white_list_final['Unified_url'],'Reachable_URL':white_list_final['Reachable_URL'],\n",
    "                  'Way_back_archived':white_list_final['Way_back_archived'],\n",
    "                   \"Freenom_top_level_domain\":white_list_final['Freenom_top_level_domain'],\n",
    "                   \"Previous_malicious_top_level_domain_TLD\":white_list_final['Previous_malicious_top_level_domain_TLD'],\n",
    "                   \"Name_length\":white_list_final['Name_length'],\"Wrong_spell_List\":white_list_final['Wrong_spell_List'],\"Longest_word_ratio\":white_list_final['Longest_word_ratio'],\n",
    "                   \"Special_mark\":white_list_final['Special_mark'],\"sub_domain\":white_list_final['sub_domain'],\n",
    "                   'Contain_Weried_number_combination':white_list_final['Contain_Weried_number_combination'],\n",
    "                   'levenshtein_distance':white_list_final['levenshtein_distance'],'Alexa_rank':white_list_final['Alexa_rank'],'Status_code':white_list_final['Status_code'],\"start_date\":white_list_final['start_date'],\n",
    "                   \"wildcard_subdomain\":white_list_final['wildcard_subdomain'],\n",
    "                  'Redirect_URL':white_list_final['Redirect_URL'],\n",
    "                  'Created on 2020':white_list_final['Created on 2020']}\n",
    "sample_data_Bing  = pd.DataFrame(dict_phish_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 680 entries, 0 to 679\n",
      "Data columns (total 18 columns):\n",
      "Unified_url                                680 non-null object\n",
      "Reachable_URL                              680 non-null int64\n",
      "Way_back_archived                          680 non-null int64\n",
      "Freenom_top_level_domain                   680 non-null int64\n",
      "Previous_malicious_top_level_domain_TLD    680 non-null int64\n",
      "Name_length                                680 non-null int64\n",
      "Wrong_spell_List                           680 non-null int64\n",
      "Longest_word_ratio                         680 non-null float64\n",
      "Special_mark                               680 non-null int64\n",
      "sub_domain                                 680 non-null int64\n",
      "Contain_Weried_number_combination          680 non-null int64\n",
      "levenshtein_distance                       680 non-null int64\n",
      "Alexa_rank                                 680 non-null int64\n",
      "Status_code                                680 non-null int64\n",
      "start_date                                 680 non-null int64\n",
      "wildcard_subdomain                         680 non-null int64\n",
      "Redirect_URL                               680 non-null int64\n",
      "Created on 2020                            680 non-null int64\n",
      "dtypes: float64(1), int64(16), object(1)\n",
      "memory usage: 95.8+ KB\n"
     ]
    }
   ],
   "source": [
    "sample_data_Bing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_Bing.to_csv('data_Confirmed_whitelist_final_version3.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549 entries, 0 to 548\n",
      "Data columns (total 18 columns):\n",
      "Unified_url                                549 non-null object\n",
      "Reachable_URL                              549 non-null int64\n",
      "Way_back_archived                          549 non-null int64\n",
      "Freenom_top_level_domain                   549 non-null int64\n",
      "Previous_malicious_top_level_domain_TLD    549 non-null int64\n",
      "Name_length                                549 non-null int64\n",
      "Wrong_spell_List                           549 non-null int64\n",
      "Longest_word_ratio                         549 non-null float64\n",
      "Special_mark                               549 non-null int64\n",
      "sub_domain                                 549 non-null int64\n",
      "Contain_Weried_number_combination          549 non-null int64\n",
      "levenshtein_distance                       549 non-null int64\n",
      "Alexa_rank                                 549 non-null int64\n",
      "Status_code                                549 non-null int64\n",
      "start_date                                 549 non-null int64\n",
      "wildcard_subdomain                         549 non-null int64\n",
      "Redirect_URL                               549 non-null int64\n",
      "Created on 2020                            549 non-null int64\n",
      "dtypes: float64(1), int64(16), object(1)\n",
      "memory usage: 77.3+ KB\n"
     ]
    }
   ],
   "source": [
    "confirmed_maclious_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 680 entries, 0 to 679\n",
      "Data columns (total 18 columns):\n",
      "Alexa_rank                                 680 non-null int64\n",
      "Contain_Weried_number_combination          680 non-null int64\n",
      "Created on 2020                            680 non-null int64\n",
      "Freenom_top_level_domain                   680 non-null int64\n",
      "Longest_word_ratio                         680 non-null float64\n",
      "Name_length                                680 non-null int64\n",
      "Previous_malicious_top_level_domain_TLD    680 non-null int64\n",
      "Reachable_URL                              680 non-null int64\n",
      "Redirect_URL                               680 non-null int64\n",
      "Special_mark                               680 non-null int64\n",
      "Status_code                                680 non-null int64\n",
      "Unified_url                                680 non-null object\n",
      "Way_back_archived                          680 non-null int64\n",
      "Wrong_spell_List                           680 non-null int64\n",
      "levenshtein_distance                       680 non-null int64\n",
      "start_date                                 680 non-null int64\n",
      "sub_domain                                 680 non-null int64\n",
      "wildcard_subdomain                         680 non-null int64\n",
      "dtypes: float64(1), int64(16), object(1)\n",
      "memory usage: 95.8+ KB\n"
     ]
    }
   ],
   "source": [
    "white_list_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "unlabeled_final = pd.read_csv('unlabled_data_final_version3.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "confirmed_maclious_final = pd.read_csv('data_confirmed_CovidURL_final_version3.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "white_list_final = pd.read_csv('data_Confirmed_whitelist_final_version3.csv',sep=',',encoding = \"ISO-8859-1\")\n",
    "Bing_final = pd.read_csv('Bing_final_version.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = [] \n",
    "temp2 = []\n",
    "for i in range(len(confirmed_maclious_final)):\n",
    "    temp1.append(1)\n",
    "for i in range(len(white_list_final)):\n",
    "    temp2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_maclious_final['label'] = temp1\n",
    "white_list_final['label'] = temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unified_url</th>\n",
       "      <th>Reachable_URL</th>\n",
       "      <th>Way_back_archived</th>\n",
       "      <th>Freenom_top_level_domain</th>\n",
       "      <th>Previous_malicious_top_level_domain_TLD</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Wrong_spell_List</th>\n",
       "      <th>Longest_word_ratio</th>\n",
       "      <th>Special_mark</th>\n",
       "      <th>sub_domain</th>\n",
       "      <th>Contain_Weried_number_combination</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>Alexa_rank</th>\n",
       "      <th>Status_code</th>\n",
       "      <th>start_date</th>\n",
       "      <th>wildcard_subdomain</th>\n",
       "      <th>Redirect_URL</th>\n",
       "      <th>Created on 2020</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://COVID--19-shop.rf.gd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>503</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://actualisatie.updateics-covid19.noez.me</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>503</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://advancedaesthetics.ch</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://aide-covid19.tn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>403</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://amazon.co.jp.initiativescompany-news-co...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>503</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Unified_url  Reachable_URL  \\\n",
       "0                        http://COVID--19-shop.rf.gd              0   \n",
       "1      http://actualisatie.updateics-covid19.noez.me              0   \n",
       "2                       http://advancedaesthetics.ch              1   \n",
       "3                             http://aide-covid19.tn              0   \n",
       "4  http://amazon.co.jp.initiativescompany-news-co...              0   \n",
       "\n",
       "   Way_back_archived  Freenom_top_level_domain  \\\n",
       "0                  0                         0   \n",
       "1                  0                         0   \n",
       "2                  0                         0   \n",
       "3                  0                         0   \n",
       "4                  0                         0   \n",
       "\n",
       "   Previous_malicious_top_level_domain_TLD  Name_length  Wrong_spell_List  \\\n",
       "0                                        0            7                 0   \n",
       "1                                        0           12                 0   \n",
       "2                                        0            4                 0   \n",
       "3                                        0            6                 0   \n",
       "4                                        0           20                 0   \n",
       "\n",
       "   Longest_word_ratio  Special_mark  sub_domain  \\\n",
       "0            0.080000             1           2   \n",
       "1            0.046512             1           3   \n",
       "2            0.076923             0           1   \n",
       "3            0.100000             1           1   \n",
       "4            0.023256             1           4   \n",
       "\n",
       "   Contain_Weried_number_combination  levenshtein_distance  Alexa_rank  \\\n",
       "0                                  0                     0           1   \n",
       "1                                  0                     0           0   \n",
       "2                                  0                     0           0   \n",
       "3                                  0                     0           1   \n",
       "4                                  1                     0           0   \n",
       "\n",
       "   Status_code  start_date  wildcard_subdomain  Redirect_URL  Created on 2020  \\\n",
       "0          503        1992                   1             0                0   \n",
       "1          503        2007                   0             0                0   \n",
       "2          200        1987                   0             0                0   \n",
       "3          403        1991                   0             0                0   \n",
       "4          503        2013                   0             0                0   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmed_maclious_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unified_url</th>\n",
       "      <th>Reachable_URL</th>\n",
       "      <th>Way_back_archived</th>\n",
       "      <th>Freenom_top_level_domain</th>\n",
       "      <th>Previous_malicious_top_level_domain_TLD</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Wrong_spell_List</th>\n",
       "      <th>Longest_word_ratio</th>\n",
       "      <th>Special_mark</th>\n",
       "      <th>sub_domain</th>\n",
       "      <th>Contain_Weried_number_combination</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>Alexa_rank</th>\n",
       "      <th>Status_code</th>\n",
       "      <th>start_date</th>\n",
       "      <th>wildcard_subdomain</th>\n",
       "      <th>Redirect_URL</th>\n",
       "      <th>Created on 2020</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>www2.gnb.ca</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>61648</td>\n",
       "      <td>200</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>www2.gov.bc.ca</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5114</td>\n",
       "      <td>200</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>www2.hse.ie</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>38132</td>\n",
       "      <td>200</td>\n",
       "      <td>1988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>yukon.ca</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>324868</td>\n",
       "      <td>200</td>\n",
       "      <td>1988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>zimnoch.smugmug.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4820</td>\n",
       "      <td>200</td>\n",
       "      <td>1988</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unified_url  Reachable_URL  Way_back_archived  \\\n",
       "675          www2.gnb.ca              1                  1   \n",
       "676       www2.gov.bc.ca              1                  1   \n",
       "677          www2.hse.ie              1                  1   \n",
       "678             yukon.ca              1                  1   \n",
       "679  zimnoch.smugmug.com              1                  0   \n",
       "\n",
       "     Freenom_top_level_domain  Previous_malicious_top_level_domain_TLD  \\\n",
       "675                         0                                        0   \n",
       "676                         0                                        0   \n",
       "677                         0                                        0   \n",
       "678                         0                                        0   \n",
       "679                         0                                        0   \n",
       "\n",
       "     Name_length  Wrong_spell_List  Longest_word_ratio  Special_mark  \\\n",
       "675            5                 0            0.222222             0   \n",
       "676            5                 0            0.166667             0   \n",
       "677            4                 0            0.222222             0   \n",
       "678            2                 0            0.333333             0   \n",
       "679            6                 0            0.117647             0   \n",
       "\n",
       "     sub_domain  Contain_Weried_number_combination  levenshtein_distance  \\\n",
       "675           2                                  0                     6   \n",
       "676           3                                  0                     7   \n",
       "677           2                                  0                     6   \n",
       "678           1                                  0                     6   \n",
       "679           2                                  0                    12   \n",
       "\n",
       "     Alexa_rank  Status_code  start_date  wildcard_subdomain  Redirect_URL  \\\n",
       "675       61648          200        1987                   0             0   \n",
       "676        5114          200        1987                   0             0   \n",
       "677       38132          200        1988                   0             0   \n",
       "678      324868          200        1988                   0             0   \n",
       "679        4820          200        1988                   1             0   \n",
       "\n",
       "     Created on 2020  label  \n",
       "675                0      0  \n",
       "676                0      0  \n",
       "677                0      0  \n",
       "678                0      0  \n",
       "679                0      0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_list_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_test_data = confirmed_maclious_final.append(white_list_final,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1229 entries, 0 to 1228\n",
      "Data columns (total 19 columns):\n",
      "Unified_url                                1229 non-null object\n",
      "Reachable_URL                              1229 non-null int64\n",
      "Way_back_archived                          1229 non-null int64\n",
      "Freenom_top_level_domain                   1229 non-null int64\n",
      "Previous_malicious_top_level_domain_TLD    1229 non-null int64\n",
      "Name_length                                1229 non-null int64\n",
      "Wrong_spell_List                           1229 non-null int64\n",
      "Longest_word_ratio                         1229 non-null float64\n",
      "Special_mark                               1229 non-null int64\n",
      "sub_domain                                 1229 non-null int64\n",
      "Contain_Weried_number_combination          1229 non-null int64\n",
      "levenshtein_distance                       1229 non-null int64\n",
      "Alexa_rank                                 1229 non-null int64\n",
      "Status_code                                1229 non-null int64\n",
      "start_date                                 1229 non-null int64\n",
      "wildcard_subdomain                         1229 non-null int64\n",
      "Redirect_URL                               1229 non-null int64\n",
      "Created on 2020                            1229 non-null int64\n",
      "label                                      1229 non-null int64\n",
      "dtypes: float64(1), int64(17), object(1)\n",
      "memory usage: 182.6+ KB\n"
     ]
    }
   ],
   "source": [
    "Sample_test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_test_data = Sample_test_data.drop(columns = ['Unified_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reachable_URL</th>\n",
       "      <th>Way_back_archived</th>\n",
       "      <th>Freenom_top_level_domain</th>\n",
       "      <th>Previous_malicious_top_level_domain_TLD</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Wrong_spell_List</th>\n",
       "      <th>Longest_word_ratio</th>\n",
       "      <th>Special_mark</th>\n",
       "      <th>sub_domain</th>\n",
       "      <th>Contain_Weried_number_combination</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>Alexa_rank</th>\n",
       "      <th>Status_code</th>\n",
       "      <th>start_date</th>\n",
       "      <th>wildcard_subdomain</th>\n",
       "      <th>Redirect_URL</th>\n",
       "      <th>Created on 2020</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>503</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>503</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>403</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>503</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Reachable_URL  Way_back_archived  Freenom_top_level_domain  \\\n",
       "0              0                  0                         0   \n",
       "1              0                  0                         0   \n",
       "2              1                  0                         0   \n",
       "3              0                  0                         0   \n",
       "4              0                  0                         0   \n",
       "\n",
       "   Previous_malicious_top_level_domain_TLD  Name_length  Wrong_spell_List  \\\n",
       "0                                        0            7                 0   \n",
       "1                                        0           12                 0   \n",
       "2                                        0            4                 0   \n",
       "3                                        0            6                 0   \n",
       "4                                        0           20                 0   \n",
       "\n",
       "   Longest_word_ratio  Special_mark  sub_domain  \\\n",
       "0            0.080000             1           2   \n",
       "1            0.046512             1           3   \n",
       "2            0.076923             0           1   \n",
       "3            0.100000             1           1   \n",
       "4            0.023256             1           4   \n",
       "\n",
       "   Contain_Weried_number_combination  levenshtein_distance  Alexa_rank  \\\n",
       "0                                  0                     0           1   \n",
       "1                                  0                     0           0   \n",
       "2                                  0                     0           0   \n",
       "3                                  0                     0           1   \n",
       "4                                  1                     0           0   \n",
       "\n",
       "   Status_code  start_date  wildcard_subdomain  Redirect_URL  Created on 2020  \\\n",
       "0          503        1992                   1             0                0   \n",
       "1          503        2007                   0             0                0   \n",
       "2          200        1987                   0             0                0   \n",
       "3          403        1991                   0             0                0   \n",
       "4          503        2013                   0             0                0   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = Sample_test_data.drop(columns = ['label','levenshtein_distance','Special_mark'])\n",
    "label = Sample_test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Train, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------TREE---------Evaluate----------------\n",
      "accuracy is  0.9310344827586207\n",
      "precision is 0.9310344827586207\n",
      "recall is 0.9310344827586207\n",
      "f1 score is 0.9308313862928348\n",
      "\n",
      "[[200  13]\n",
      " [ 15 178]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from skrules import SkopeRules\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_fscore_support,accuracy_score, precision_score,recall_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def Decision_tree(X_train, X_test, y_train, y_test):\n",
    "    Tree = tree.DecisionTreeClassifier(criterion='gini')\n",
    "    Tree.fit(X_train, y_train)\n",
    "    prediction = Tree.predict(X_test)\n",
    "    print(\"------TREE---------Evaluate----------------\")\n",
    "    recall = recall_score(y_test, prediction,average='micro')\n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    precision = precision_score(y_test, prediction,average='micro')\n",
    "    f1 = f1_score(y_test, prediction, average='macro')\n",
    "    print(\"accuracy is \", accuracy)\n",
    "    print(\"precision is\", precision)\n",
    "    print(\"recall is\", recall)\n",
    "    print(\"f1 score is\",f1)\n",
    "    print('')\n",
    "    confusion_matrix1 = confusion_matrix(y_test, prediction, labels=[0, 1])\n",
    "    print(confusion_matrix1)    \n",
    "    return accuracy, precision, recall,f1_score\n",
    "accuracy_DT, precision_DT, recall_DT,f1_score = Decision_tree(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------KNN---------Evaluate----------------\n",
      "accuracy is  0.9236453201970444\n",
      "precision is 0.9236453201970444\n",
      "recall is 0.9236453201970444\n",
      "f1 score is 0.9236226158616881\n",
      "\n",
      "[[191  22]\n",
      " [  9 184]]\n"
     ]
    }
   ],
   "source": [
    "def KNN(train_x, x_test, train_y, y_test):\n",
    "    nbrs = KNeighborsClassifier(n_neighbors=14,weights='distance',p=2)\n",
    "    nbrs.fit(train_x, train_y)\n",
    "    prediction = nbrs.predict(x_test)\n",
    "    print(\"------KNN---------Evaluate----------------\")\n",
    "    recall = recall_score(y_test, prediction,average='micro')\n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    precision = precision_score(y_test, prediction,average='micro')\n",
    "    print(\"accuracy is \", accuracy)\n",
    "    print(\"precision is\", precision)\n",
    "    print(\"recall is\", recall)\n",
    "    f1 = f1_score(y_test, prediction, average='macro')\n",
    "    print(\"f1 score is\",f1)\n",
    "    print('')\n",
    "    confusion_matrix1 = confusion_matrix(y_test, prediction, labels=[0, 1])\n",
    "    print(confusion_matrix1)    \n",
    "    return accuracy, precision, recall, f1 \n",
    "accuracy_KNN, precision_KNN, recall_KNN,f1_score_KNN = KNN(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------RandomForest---------Evaluate----------------\n",
      "cross score is [0.95180723 0.95180723 0.97590361 0.95121951 0.96341463 0.96341463\n",
      " 0.96341463 0.95121951 0.95121951 0.92682927]\n",
      "accuracy is  0.9482758620689655\n",
      "precision is 0.9482758620689655\n",
      "recall is 0.9482758620689655\n",
      "f1 score is 0.9481623347013224\n",
      "\n",
      "[[202  11]\n",
      " [ 10 183]]\n"
     ]
    }
   ],
   "source": [
    "def RandomForest(train_x, x_test, train_y, y_test):\n",
    "    clf = RandomForestClassifier(n_estimators=100,max_depth=None, max_features='auto')\n",
    "    clf.fit(train_x, train_y)\n",
    "    prediction = clf.predict(x_test)\n",
    "    print(\"------RandomForest---------Evaluate----------------\")\n",
    "    recall = recall_score(y_test, prediction,average='micro')\n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    cross_val = cross_val_score(clf, train_x, train_y, cv=10,scoring='accuracy')\n",
    "    precision = precision_score(y_test, prediction,average='micro')\n",
    "    print('cross score is', cross_val)\n",
    "    print(\"accuracy is \", accuracy)\n",
    "    print(\"precision is\", precision)\n",
    "    print(\"recall is\", recall)\n",
    "    f1 = f1_score(y_test, prediction, average='macro')\n",
    "    print(\"f1 score is\",f1)\n",
    "    print('')\n",
    "    confusion_matrix1 = confusion_matrix(y_test, prediction, labels=[0, 1])\n",
    "    print(confusion_matrix1)\n",
    "    return accuracy, precision, recall, f1 \n",
    "\n",
    "accuracy_RF, precision_RF, recall_RF,f1_score_RF = RandomForest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------BAYES---------Evaluate----------------\n",
      "cross score is [0.77108434 0.81927711 0.84337349 0.79268293 0.85365854 0.79268293\n",
      " 0.80487805 0.87804878 0.82926829 0.82926829]\n",
      "accuracy is  0.7980295566502463\n",
      "precision is 0.7980295566502463\n",
      "recall is 0.7980295566502463\n",
      "f1 score is 0.7980295566502463\n",
      "\n",
      "[[162  51]\n",
      " [ 31 162]]\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(train_x, x_test, train_y, y_test):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(train_x, train_y)\n",
    "    prediction = clf.predict(x_test)\n",
    "    print(\"------BAYES---------Evaluate----------------\")\n",
    "    recall = recall_score(y_test, prediction,average='micro')\n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    cross_val = cross_val_score(clf, train_x, train_y, cv=10,scoring='accuracy')\n",
    "    precision = precision_score(y_test, prediction,average='micro')\n",
    "    print('cross score is', cross_val)\n",
    "    print(\"accuracy is \", accuracy)\n",
    "    print(\"precision is\", precision)\n",
    "    print(\"recall is\", recall)\n",
    "    f1 = f1_score(y_test, prediction, average='macro')\n",
    "    print(\"f1 score is\",f1)\n",
    "    print('')\n",
    "    confusion_matrix1 = confusion_matrix(y_test, prediction, labels=[0, 1])\n",
    "    print(confusion_matrix1)    \n",
    "    return accuracy, precision, recall, cross_val\n",
    "\n",
    "accuracy_NB, precision_NB, recall_NB,f1_score_NB = naive_bayes(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(823, 15)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 200)               3200      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 23,401\n",
      "Trainable params: 23,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 4890765.0000 - accuracy: 0.5200\n",
      "Epoch 2/30\n",
      "9/9 [==============================] - 0s 777us/step - loss: 68.6967 - accuracy: 0.7533\n",
      "Epoch 3/30\n",
      "9/9 [==============================] - 0s 833us/step - loss: 27.6180 - accuracy: 0.7667\n",
      "Epoch 4/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 22.4942 - accuracy: 0.7667\n",
      "Epoch 5/30\n",
      "9/9 [==============================] - 0s 812us/step - loss: 16.8663 - accuracy: 0.7971\n",
      "Epoch 6/30\n",
      "9/9 [==============================] - 0s 835us/step - loss: 12.9894 - accuracy: 0.7861\n",
      "Epoch 7/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 10.7812 - accuracy: 0.7910\n",
      "Epoch 8/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 8.8744 - accuracy: 0.8032\n",
      "Epoch 9/30\n",
      "9/9 [==============================] - 0s 983us/step - loss: 9.7319 - accuracy: 0.7776\n",
      "Epoch 10/30\n",
      "9/9 [==============================] - 0s 990us/step - loss: 7.8711 - accuracy: 0.8044\n",
      "Epoch 11/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 7.0805 - accuracy: 0.7910\n",
      "Epoch 12/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 6.5489 - accuracy: 0.8019\n",
      "Epoch 13/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 5.6251 - accuracy: 0.8080\n",
      "Epoch 14/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 5.3890 - accuracy: 0.8104\n",
      "Epoch 15/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 5.2196 - accuracy: 0.7922\n",
      "Epoch 16/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 4.3954 - accuracy: 0.8007\n",
      "Epoch 17/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 4.3438 - accuracy: 0.8153\n",
      "Epoch 18/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 3.3413 - accuracy: 0.7995\n",
      "Epoch 19/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 4.2385 - accuracy: 0.7922\n",
      "Epoch 20/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 3.9867 - accuracy: 0.7983\n",
      "Epoch 21/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 3.1313 - accuracy: 0.7995\n",
      "Epoch 22/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.5101 - accuracy: 0.8092\n",
      "Epoch 23/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 3.1416 - accuracy: 0.7922\n",
      "Epoch 24/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.7206 - accuracy: 0.7910\n",
      "Epoch 25/30\n",
      "9/9 [==============================] - 0s 975us/step - loss: 2.3846 - accuracy: 0.8104\n",
      "Epoch 26/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.9335 - accuracy: 0.8250\n",
      "Epoch 27/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.1040 - accuracy: 0.8044\n",
      "Epoch 28/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.9080 - accuracy: 0.8092\n",
      "Epoch 29/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 2.3809 - accuracy: 0.8117\n",
      "Epoch 30/30\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.8962 - accuracy: 0.8165\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import keras\n",
    "batch_size = 100\n",
    "epochs = 30\n",
    "model = Sequential()\n",
    "model.add(Dense(200, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "opt = keras.optimizers.SGD(lr=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,                   \n",
    "                    epochs=epochs,\n",
    "                    verbose=1)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_mlp = []\n",
    "for i in prediction:\n",
    "    prediction_mlp.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------MLP---------Evaluate----------------\n",
      "accuracy is  0.8694581280788177\n",
      "precision is 0.8694581280788177\n",
      "recall is 0.8694581280788177\n",
      "f1 score is 0.869438326325306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = prediction_mlp\n",
    "print(\"------MLP---------Evaluate----------------\")\n",
    "recall = recall_score(y_test, prediction,average='micro')\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "precision = precision_score(y_test, prediction,average='micro')\n",
    "print(\"accuracy is \", accuracy)\n",
    "print(\"precision is\", precision)\n",
    "print(\"recall is\", recall)\n",
    "f1 = f1_score(y_test, prediction, average='macro')\n",
    "print(\"f1 score is\",f1)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
