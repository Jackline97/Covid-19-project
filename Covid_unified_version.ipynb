{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "Data_afterAPI = pd.read_csv('data_with_crawler.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_afterAPI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unified_url = Data_afterAPI['Unified_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja\n",
    "from math import log\n",
    "def Extract_FTLD(List_all):\n",
    "    Freenom_top_level_domain = []\n",
    "    for i in List_all:\n",
    "        if i[-2:] == 'ml':\n",
    "            Freenom_top_level_domain.append(1)\n",
    "        elif i[-2:] == 'cf':\n",
    "            Freenom_top_level_domain.append(1)\n",
    "        elif i[-2:] == 'gq':\n",
    "            Freenom_top_level_domain.append(1)\n",
    "        elif i[-2:] == 'tk':\n",
    "            Freenom_top_level_domain.append(1)\n",
    "        elif i[-2:] == 'ga':\n",
    "            Freenom_top_level_domain.append(1)\n",
    "        else:\n",
    "            Freenom_top_level_domain.append(0)\n",
    "    return Freenom_top_level_domain\n",
    "\n",
    "def Previous_MTLD(List_all):\n",
    "# According to https://blogs.akamai.com/2019/10/a-view-into-top-level-domain-tld-abuse.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+TheAkamaiBlog+%28The+Akamai+Blog%29\n",
    "# According to https://www.anomali.com/blog/abusing-the-mali-cctld-ml-to-target-dutch-organisations\n",
    "    Previous_malicious_top_level_domain_TLD = []\n",
    "    for i in List_all:\n",
    "        if i[-2:] == 'ml':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'cf':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'so':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-4:] == 'loan':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-5:] == 'tokyo':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-5:] == 'trade':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-6:] == 'stream':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-3:] == 'bid':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-3:] == 'icu':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-3:] == 'gdn':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-3:] == 'win':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-4:] == 'work':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-4:] == 'desi':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-4:] == 'pics':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'gq':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'tk':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'vg':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'ga':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'to':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'cc':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'hk':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'pw':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'fm':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        elif i[-2:] == 'la':\n",
    "            Previous_malicious_top_level_domain_TLD.append(1)\n",
    "        else:\n",
    "            Previous_malicious_top_level_domain_TLD.append(0)\n",
    "    return Previous_malicious_top_level_domain_TLD\n",
    "\n",
    "def mixed_feature(List_all):\n",
    "    # Build a cost dictionary, assuming Zipf's law and cost = -math.log(probability).\n",
    "    # words = open(\"words-by-frequency.txt\").read().split()\n",
    "    # wordcost = dict((k, log((i+1)*log(len(words)))) for i,k in enumerate(words))\n",
    "    # maxword = max(len(x) for x in words)\n",
    "\n",
    "    def infer_spaces(s):\n",
    "        \"\"\"Uses dynamic programming to infer the location of spaces in a string\n",
    "        without spaces.\"\"\"\n",
    "\n",
    "        # Find the best match for the i first characters, assuming cost has\n",
    "        # been built for the i-1 first characters.\n",
    "        # Returns a pair (match_cost, match_length).\n",
    "        def best_match(i):\n",
    "            candidates = enumerate(reversed(cost[max(0, i-maxword):i]))\n",
    "            return min((c + wordcost.get(s[i-k-1:i], 9e999), k+1) for k,c in candidates)\n",
    "\n",
    "        # Build the cost array.\n",
    "        cost = [0]\n",
    "        for i in range(1,len(s)+1):\n",
    "            c,k = best_match(i)\n",
    "            cost.append(c)\n",
    "\n",
    "        # Backtrack to recover the minimal-cost string.\n",
    "        out = []\n",
    "        i = len(s)\n",
    "        while i>0:\n",
    "            c,k = best_match(i)\n",
    "            assert c == cost[i]\n",
    "            out.append(s[i-k:i])\n",
    "            i -= k\n",
    "\n",
    "        return \" \".join(reversed(out))\n",
    "\n",
    "    # This would take some time\n",
    "    word_dic = []\n",
    "    number_mark = []\n",
    "    Name_length = []\n",
    "    Wrong_spell = [\"cov1d\",\"c0v1d\",\"c0vid\",\"c0rona\",\"c0r0na\",\"cor0na\",\"v1rus\",\"coivd\",'co1vd']\n",
    "    Wrong_spell = Wrong_spell \n",
    "    Wrong_spell_List = []\n",
    "\n",
    "    for i in List_all:\n",
    "    #     reduce the prefix and sufix\n",
    "        temp_mark = 0\n",
    "        for j in Wrong_spell:\n",
    "            if j in i:\n",
    "    #             print(j)\n",
    "                temp_mark += 1          \n",
    "        if temp_mark>0:\n",
    "            Wrong_spell_List.append(1)\n",
    "        else:\n",
    "            Wrong_spell_List.append(0)\n",
    "        original_len = len(i) - 2 \n",
    "        Each_List = wordninja.split(i)\n",
    "        Name_length.append(len(Each_List))\n",
    "        stand = 0\n",
    "        for j in Each_List:\n",
    "            if str.isdigit(j) is True:\n",
    "                number_mark.append(1)\n",
    "                stand = 1\n",
    "            break\n",
    "        if stand==0:\n",
    "            number_mark.append(0)\n",
    "        longest_element = max([(len(x),x) for x in Each_List])\n",
    "        Ratio = len(longest_element)/original_len\n",
    "        word_dic.append(Ratio)\n",
    "    print(\"The produced word ratio list's length is \",len(word_dic))\n",
    "    print(\"The sample data is\",word_dic[0:6])\n",
    "    return Name_length, Wrong_spell_List,word_dic\n",
    "\n",
    "def Find_Dash_mark(List_all):\n",
    "    Special_mark = []\n",
    "    for i in List_all:\n",
    "        if '-' in i:\n",
    "    #         print(i)\n",
    "            Special_mark.append(1)\n",
    "        else:\n",
    "            Special_mark.append(0)\n",
    "    print(\"The number of domains cotain symbol '-':\",Special_mark.count(1))\n",
    "    return Special_mark\n",
    "    \n",
    "def Count_subdomain(List_all):\n",
    "    sub_domain = []\n",
    "    for i in List_all:\n",
    "        dot_num = i.count('.') \n",
    "        sub_domain.append(dot_num)\n",
    "    return sub_domain\n",
    "\n",
    "def Contain_IP_address(List_all):\n",
    "    import re\n",
    "    Contain_IP_Adress = []\n",
    "    for i in List_all:\n",
    "        IP = re.findall(r\".\\d.\\d.\\d\",i)\n",
    "        if len(IP) == 0:\n",
    "            Contain_IP_Adress.append(0)\n",
    "        else:\n",
    "            Contain_IP_Adress.append(1)\n",
    "    print(\"The number of domain contain IP adress is\",Contain_IP_Adress.count(1))\n",
    "    return Contain_IP_Adress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freenom_top_level_domain = Extract_FTLD(all_unified_url)\n",
    "Previous_malicious_top_level_domain_TLD = Previous_MTLD(all_unified_url)\n",
    "Name_length, Wrong_spell_List,word_dic = mixed_feature(all_unified_url)\n",
    "Special_mark = Find_Dash_mark(all_unified_url)\n",
    "sub_domain = Count_subdomain(all_unified_url)\n",
    "Contain_IP_Adress = Contain_IP_address(all_unified_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://158.69.153.253\n",
      "http://19covid-gouv12.com\n",
      "http://3dforcovid.com\n",
      "http://COVID--19-shop.rf.gd\n",
      "http://actualisatie.updateics-covid19.noez.me\n",
      "http://advancedaesthetics.ch\n",
      "http://aide-covid19.tn\n",
      "http://airbnb.id-covid19.com\n",
      "http://amazon.co.jp.initiativescompany-news-covid19-20200316token4787f6f3e205921236.buzz\n",
      "http://amazon.co.jp.initiativescompany-news-covid19-20200316token4787f6f3e205923534.buzz\n",
      "http://amazon.co.jp.initiativescompany-news-covid19-20200316token4787f6f3e205926666.buzz\n",
      "http://amelenedez.com\n",
      "http://amelzendez.com\n",
      "http://americanascovidshop.com\n",
      "http://amqelendez.com\n",
      "http://andreakristi.000webhostapp.com\n",
      "http://anti-coronavirus.sk\n",
      "http://anti-covid.sk\n",
      "http://anti-covid.supplies\n",
      "http://anti-covid19.sk\n",
      "http://anticovid-19.sk\n",
      "http://artscovid.org\n",
      "http://aruba-covid19.u720493s8q.ha004.t.justns.ru\n",
      "http://ateaseoutfitters.com\n",
      "http://atlantabeatscovid.com\n",
      "http://atlantabeatscovid.org\n",
      "http://auxilioemergencialcovid.com\n",
      "http://ayudadigitalcovid.org\n",
      "http://ayyappantat.com\n",
      "http://bantaycovid.com\n",
      "http://barcelonaneoville.com.br\n",
      "http://bb-iscoronas.it\n",
      "http://beccahorner.com\n",
      "http://bell-covid19.com\n",
      "http://berita-covid-19-terkini-tasikmalaya.000webhostapp.com\n",
      "http://bhleon.com-do.serfidelo.com\n",
      "http://bit.ly\n",
      "http://bizrak.com\n",
      "http://bodicia.com.ru\n",
      "http://bokepspecialcovid19.event-indo.com\n",
      "http://bokepviral.19covidd.com\n",
      "http://bp-formularios-covid19.azurewebsites.net\n",
      "http://bravecovid.com\n",
      "http://business-facebook-covid19.com\n",
      "http://caninetags.ga\n",
      "http://carrefourcovid.com\n",
      "http://cbsrealitychase.com\n",
      "http://cdestudiantes.com\n",
      "http://cecollc.com\n",
      "http://chase-covid19s.com\n",
      "http://chase7-covid.com\n",
      "http://chasecovid19s.com\n",
      "http://chasecovid19t.com\n",
      "http://chasecovid19v.com\n",
      "http://cherrydancefitness.co.uk\n",
      "http://childcarecorona.com\n",
      "http://chirurgiehmrcovid.com\n",
      "http://cinenagari.com\n",
      "http://click.cocovid.co\n",
      "http://cmattayers.com\n",
      "http://cocovid.co\n",
      "http://combatecovid.org\n",
      "http://computecovid.com\n",
      "http://copingwithcovid.blog\n",
      "http://corona-live.net\n",
      "http://corona-masr2.com\n",
      "http://corona-masr21.com\n",
      "http://corona-masr3.com\n",
      "http://corona-nearby.com\n",
      "http://corona-virus-anonymous.org\n",
      "http://corona-virusus.com\n",
      "http://corona19covid.000webhostapp.com\n",
      "http://coronabye.com\n",
      "http://coronadocruisers.com\n",
      "http://coronaent.com.hk\n",
      "http://coronaextra.com.au\n",
      "http://coronaflexx.000webhostapp.com\n",
      "http://coronagame1.co.vu\n",
      "http://coronagame2.co.vu\n",
      "http://coronagame3.co.vu\n",
      "http://coronagame4.co.vu\n",
      "http://coronamask.space\n",
      "http://coronanow.kr\n",
      "http://coronasmask.space\n",
      "http://coronate-sounds.000webhostapp.com\n",
      "http://coronation.ml\n",
      "http://coronatoken.org\n",
      "http://coronavgame1.atwebpages.com\n",
      "http://coronavgame2.atwebpages.com\n",
      "http://coronavgame3.atwebpages.com\n",
      "http://coronavirus-help.uk\n",
      "http://coronavirus-in.space\n",
      "http://coronavirus-news.org.uk\n",
      "http://coronavirus-v-mire.ru\n",
      "http://coronavirusbacktoreality.com\n",
      "http://coronavirusfactory.com\n",
      "http://coronavirusfeedback.com\n",
      "http://coronavirusfinancial.net\n",
      "http://coronavirushazard.com\n",
      "http://coronavirusinasec.000webhostapp.com\n",
      "http://coronavirusnews.000webhostapp.com\n",
      "http://coronavirusonooeir.huidps.com\n",
      "http://coronavirusonooeir.luxciosoiop.com\n",
      "http://coronavirussms.com\n",
      "http://coronavirustrouble.com\n",
      "http://coronavirusu.info\n",
      "http://coronawatch.eu\n",
      "http://coronawire.com\n",
      "http://coronawire.in\n",
      "http://coronawires.com\n",
      "http://coronbeer.com\n",
      "http://cottonbeatscovid.com\n",
      "http://courseaholic.com\n",
      "http://covid--19-shop.rf.gd\n",
      "http://covid-19.bdtime.news\n",
      "http://covid-19.xxx-wa.com\n",
      "http://covid-192.godaddysites.com\n",
      "http://covid-19art.com\n",
      "http://covid-19coronavirus.co.uk\n",
      "http://covid-19finance.co.uk\n",
      "http://covid-19rc.com\n",
      "http://covid.seguranca-bb.info\n",
      "http://covid19---shop.rf.gd\n",
      "http://covid19-credits.com\n",
      "http://covid19-finance.co.uk\n",
      "http://covid19-infor.net\n",
      "http://covid19-seguranca.autosmsbb.com\n",
      "http://covid19-survei.000webhostapp.com\n",
      "http://covid19-vat.rf.gd\n",
      "http://covid19.id-airbnb.com\n",
      "http://covid19.seguranca-bb.info\n",
      "http://covid19bireyseliadeniz.com\n",
      "http://covid19evdekalaidatinial.com\n",
      "http://covid19fund.co.uk\n",
      "http://covid19healthcare.000webhostapp.com\n",
      "http://covid19hosting.co.uk\n",
      "http://covid19lssuedbill.com\n",
      "http://covid19partnership.000webhostapp.com\n",
      "http://covid19peticija.000webhostapp.com\n",
      "http://covid19retailpulselive.com\n",
      "http://covid19voip.com\n",
      "http://covid2019colpatria.com.co\n",
      "http://covidcro.tk\n",
      "http://covidguide.guru\n",
      "http://covidi9.gq\n",
      "http://covidreamz.com\n",
      "http://covidsome.com\n",
      "http://cracovidfunds.com\n",
      "http://craemrcovid19.com\n",
      "http://crookedcovid.com\n",
      "http://crushcovid.net\n",
      "http://csam-corona.be\n",
      "http://curbsidecovid.com\n",
      "http://cutt.ly\n",
      "http://dailycorona.bitbyteplay.com\n",
      "http://daneili-corus.com\n",
      "http://dati-covid19-psd2.000webhostapp.com\n",
      "http://dentalleadgroup.com\n",
      "http://descovid-19.com\n",
      "http://dlwbfire.org\n",
      "http://dostaana.ml\n",
      "http://dtipgifts.com\n",
      "http://ecolenefiber.com\n",
      "http://ecovida.ru\n",
      "http://edigishoppee.com\n",
      "http://edumetrix.io\n",
      "http://eecovid19-support150.hyundaigiadinhsaigon.com\n",
      "http://envisioncm.com\n",
      "http://escalainicial.com.ar\n",
      "http://eshop-COVID19.rf.gd\n",
      "http://eshop-covid19.rf.gd\n",
      "http://evnt.com.br\n",
      "http://faq-coronavirus-financial-help.lsign.site\n",
      "http://faq-coronavirus-financial-help.nsign.me\n",
      "http://financingcovid.com\n",
      "http://firealarmcemen.com\n",
      "http://flatheadcovid.org\n",
      "http://fmcg-patterns.com\n",
      "http://footytube.top\n",
      "http://forgecovid.com\n",
      "http://frateemedia.com\n",
      "http://fushet.com\n",
      "http://gcoronag.000webhostapp.com\n",
      "http://gelaallc.com\n",
      "http://gigaplay.com.br\n",
      "http://goovcovid19.com\n",
      "http://gouvcanada-covid19.com\n",
      "http://hackingcovid.info\n",
      "http://hansonbalirunoug.com\n",
      "http://help-corona.cn\n",
      "http://helptoavoidcoronaa.000webhostapp.com\n",
      "http://highbrowclothing.com\n",
      "http://homologacao.xocovid19.com.br\n",
      "http://id-covid19.com\n",
      "http://informecovid.com\n",
      "http://ing.csam-corona.be\n",
      "http://ing.securecovid-19.noez.me\n",
      "http://intellicovid.com\n",
      "http://intercovid.com\n",
      "http://iowacovid.com\n",
      "http://iremoscombaterocovid.ml\n",
      "http://irs-gov.uc.r.appspot.com\n",
      "http://ita-covid19.com\n",
      "http://itacontracovid-19.ml\n",
      "http://italy-covid19.u720553s9g.ha004.t.justns.ru\n",
      "http://ithinkihavecovid.com\n",
      "http://jacarandascovid.com\n",
      "http://jararandascovid.com\n",
      "http://jasminettv.com\n",
      "http://join-whatsapp-bokep62.covid99.gq\n",
      "http://jshirt.it\n",
      "http://kampcbation.info\n",
      "http://keolis-covid.com\n",
      "http://kiwiyazilim.com\n",
      "http://kylebeard.com\n",
      "http://lanzarotecovid.com\n",
      "http://laylaraephoto.com\n",
      "http://lifelinen.com\n",
      "http://magalu-combate-ao-corona.com\n",
      "http://mail.covid-19.xxx-wa.com\n",
      "http://manchoujouser.com\n",
      "http://mapacovid.com\n",
      "http://mascarillasparacovid.com\n",
      "http://masry-corona51.com\n",
      "http://mastermovesltd.com\n",
      "http://mbhydro-covid19.com\n",
      "http://mcovid.com\n",
      "http://mediterraneosantamarinella.com\n",
      "http://mersrekdocuments.ir\n",
      "http://midiaplural.com.br\n",
      "http://mlskitchensmanchester.com\n",
      "http://moabcovid.org\n",
      "http://mortgageks.com\n",
      "http://mradani.ml\n",
      "http://mygpstrip.net\n",
      "http://nellyreifler.com\n",
      "http://nepalcoronavirus2.000webhostapp.com\n",
      "http://neraretionbriesx.club\n",
      "http://netflix-covid-19.com\n",
      "http://newcovid19unread.z13.web.core.windows.net\n",
      "http://news.att-covid19.com\n",
      "http://nimbleurbia53.com\n",
      "http://no-covid.online\n",
      "http://nocovid.fun\n",
      "http://notmycovid.org\n",
      "http://nr01petitieonline-covid19byfacebook.gq\n",
      "http://nxt27893.nextadmin.hu\n",
      "http://oceansapparel.com\n",
      "http://odessavscovid.info\n",
      "http://oldschool.runescape-covid-19.info\n",
      "http://onlnne.com\n",
      "http://orecon.co.jp\n",
      "http://parismomes.fr\n",
      "http://partners-covid.org\n",
      "http://partnerscovid.org\n",
      "http://pastcovid.com\n",
      "http://peduli-corona.wikaba.com\n",
      "http://permovqu.com\n",
      "http://pescaturismocorona.it\n",
      "http://piocppocodso.blob.core.windows.net\n",
      "http://plantingvelve.com\n",
      "http://playdomain53.com\n",
      "http://pledge-againstt-corona.000webhostapp.com\n",
      "http://politicovid.com\n",
      "http://portal.auone.jp-verifykey.covid-191.com\n",
      "http://portsmouth.buzz\n",
      "http://pre-covid.info\n",
      "http://previnadocoronasuavida.com\n",
      "http://previnasedocoronavirus.com\n",
      "http://pulsagratiscorona.000webhostapp.com\n",
      "http://rcconstrutora.com.br\n",
      "http://recovid.us\n",
      "http://register-participatecoronafreecoins300.mrbonus.com\n",
      "http://register-toparticipateviruscoronafree300coin.mrbonus.com\n",
      "http://remittancefiles.com\n",
      "http://rijosfoods.com.br\n",
      "http://rogers-covid19.com\n",
      "http://runescape-covid19.info\n",
      "http://saicoronadeixanoistrabalhar.com\n",
      "http://sajayagroup.com\n",
      "http://seeannsave.com\n",
      "http://shop---covid19.rf.gd\n",
      "http://shop--covid-19.rf.gd\n",
      "http://shop--covid19.rf.gd\n",
      "http://shop-covid19.rf.gd\n",
      "http://shop-sars-covid19.rf.gd\n",
      "http://shopcovid-19.rf.gd\n",
      "http://shops-covid-19.rf.gd\n",
      "http://shopscovid19.rf.gd\n",
      "http://sicurezza-covid19.com\n",
      "http://simplyoneden.com\n",
      "http://simplysororitypackets.com\n",
      "http://skcovid.com\n",
      "http://skcovid.org\n",
      "http://solidarite-covid.org\n",
      "http://spokanecovid.com\n",
      "http://standagainstcovid.org\n",
      "http://starilionpla.website\n",
      "http://stopcorona.org\n",
      "http://stopcovid.store\n",
      "http://storage.googleapis.com\n",
      "http://storiesofcovid.com\n",
      "http://summergirlfilms.com\n",
      "http://supperbelle.com\n",
      "http://t-uber.me\n",
      "http://t-ubersa.com\n",
      "http://taikisushi.com\n",
      "http://tarjeta-bci.cf\n",
      "http://testkitcovid.net\n",
      "http://testkitcovid.org\n",
      "http://thaiaichi.co.th\n",
      "http://thechristianwardrobe.us\n",
      "http://thusdaycoronaupdae.blob.core.windows.net\n",
      "http://thxcovid.com\n",
      "http://todoscontraocovid19.com\n",
      "http://todosjuntoscontraocovid.com\n",
      "http://topbrokersrealty.icu\n",
      "http://toyswithpizzazz.com.au\n",
      "http://trackcoronavirus.com\n",
      "http://trackcovid.app\n",
      "http://trainingcanine.ga\n",
      "http://transferenciasscotiabankk-chile-covid.gq\n",
      "http://trenv.coronavideo3.tk\n",
      "http://trustedproductreview.com\n",
      "http://twcindia.com\n",
      "http://tzetta.com\n",
      "http://uberpromocovid19.000webhostapp.com\n",
      "http://uk-covid-19-relieve.com\n",
      "http://uman4covid.org\n",
      "http://us-coronavirus-cases-state-by-state.afyfr.com\n",
      "http://usavscovid.com\n",
      "http://vamosacabcomocorona.com\n",
      "http://vamoscombaterocoronanaosaiadecasa.com\n",
      "http://vergognacovid.com\n",
      "http://verizoncovid-12.com\n",
      "http://videoonlinefrecorona.xyz\n",
      "http://villacorona.pl\n",
      "http://virusscoronas.000webhostapp.com\n",
      "http://virys-covid19.ru\n",
      "http://vivadiagcovid.com\n",
      "http://volontaire-covid.com\n",
      "http://volontairescovid.com\n",
      "http://vvho-dropb0xcloud.selfip.com\n",
      "http://wearecovid.com\n",
      "http://webbfilms.co.uk\n",
      "http://wefightcovid.se\n",
      "http://welcometocoronado.com\n",
      "http://whatcoronavirus.com\n",
      "http://whereiscovid.info\n",
      "http://wintexgroup-cn.com\n",
      "http://www.4mysecure-facebookcovid19petition.cf\n",
      "http://www.airbnb.id-covid19.com\n",
      "http://www.americanascovidshop.com\n",
      "http://www.brightparcel.com\n",
      "http://www.brunoespanha.com\n",
      "http://www.cbsrealitychase.com\n",
      "http://www.cherrydancefitness.co.uk\n",
      "http://www.cine.com.uy\n",
      "http://www.coronavirusonooeir.huidps.com\n",
      "http://www.covid-19challengecoin.com\n",
      "http://www.covidvirus.guru\n",
      "http://www.dailycorona.bitbyteplay.com\n",
      "http://www.eater.com\n",
      "http://www.gov-ca-covid19.org\n",
      "http://www.iacovides.com\n",
      "http://www.im4free.com\n",
      "http://www.irs.org\n",
      "http://www.irshelpcovid19.com\n",
      "http://www.ita-covid19.com\n",
      "http://www.itaucovid19.com\n",
      "http://www.koharrhealth.com\n",
      "http://www.lamalug.org\n",
      "http://www.lovecelebrities.com\n",
      "http://www.mediterraneosantamarinella.com\n",
      "http://www.n95mask-covid19.com\n",
      "http://www.nellyreifler.com\n",
      "http://www.netflix-covid-19.com\n",
      "http://www.peduli-corona.wikaba.com\n",
      "http://www.pescaturismocorona.it\n",
      "http://www.plantingvelve.com\n",
      "http://www.promo-covid19-neftlix.ml\n",
      "http://www.register-participatecoronafreecoins300.mrbonus.com\n",
      "http://www.reguster-forparticipateinthecorona-freecoin300.mrbonus.com\n",
      "http://www.rogers-covid19.com\n",
      "http://www.stpl.ca\n",
      "http://www.tzetta.com\n",
      "http://www.wailmegensa.com\n",
      "http://www.welcometocoronado.com\n",
      "http://www.wfb-wa-identity-confirmation-now.agency\n",
      "http://www4-irs-gov.uc.r.appspot.com\n",
      "http://xetoancau.vn\n",
      "http://xocovid19.com.br\n",
      "http://zendesk-covid19.org\n",
      "http://zonasegurabeta.viabcp-covid19.\n"
     ]
    }
   ],
   "source": [
    "confirmed_phishing_website = []\n",
    "f = open(\"ConfirmedPhishing.txt\", \"r\")\n",
    "for x in f:\n",
    "    print('http://'+ x[3:-4])\n",
    "    confirmed_phishing_website.append('http://'+ x[3:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(source,target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "levenshtein_edit_dis_with_maclious = []\n",
    "for i in tqdm(all_unified_url):\n",
    "    dis_temp = []\n",
    "    for j in confirmed_phishing_website:\n",
    "        dis = levenshtein(i, j)\n",
    "        dis_temp.append(dis)\n",
    "        distance = np.mean(dis_temp)\n",
    "    levenshtein_edit_dis_with_maclious.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Freenom_top_level_domain))\n",
    "print(len(Previous_malicious_top_level_domain_TLD))\n",
    "print(len(Name_length))\n",
    "print(len(Wrong_spell_List))\n",
    "print(len(word_dic))\n",
    "print(len(Special_mark))\n",
    "print(len(sub_domain))\n",
    "print(len(Contain_IP_Adress))\n",
    "print(len(levenshtein_edit_dis_with_maclious))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_url = []\n",
    "for i in all_unified_url:\n",
    "    if i[:9] == 'http://w.':\n",
    "        modified_url.append('http://' + i[9:])\n",
    "    else:\n",
    "        modified_url.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'Unified_url':modified_url,'Reachable_URL':Data_afterAPI['Reachable_URL'],'Time_stamp_if_exist':Data_afterAPI['Time_stamp_if_exist'],'Way_back_archived':Data_afterAPI['Way_back_archived'],\"Freenom_top_level_domain\":Freenom_top_level_domain,\"Previous_malicious_top_level_domain_TLD\":Previous_malicious_top_level_domain_TLD,\"Name_length\":Name_length,\"Wrong_spell_List\":Wrong_spell_List,\"word_dic\":word_dic,\"Special_mark\":Special_mark,\"sub_domain\":sub_domain,\"Contain_IP_Adress\":Contain_IP_Adress,\"levenshtein_distance\":levenshtein_edit_dis_with_maclious}\n",
    "sample = pd.DataFrame(dict)\n",
    "sample.to_csv('data_for_now2.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modified_url[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Data_afterAPI = pd.read_csv('data_for_now2.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_url = Data_afterAPI['Unified_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "Alexa_rank = []\n",
    "Status = []\n",
    "for i in tqdm(modified_url):\n",
    "    url1 = 'https://awis.api.alexa.com/api?Action=UrlInfo&Count=10&ResponseGroup=Rank,LinksInCount&Start=1&Url='\n",
    "    url = url1+i\n",
    "    headers={'x-api-key':'R90lWSm4iC6L6zDUZnZgs8UmqmtUCFrB6fCT2EY5'}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    try:\n",
    "        statuscode = soup.findAll(\"statuscode\")[0].string\n",
    "        if statuscode is None:\n",
    "            Status.append(0)\n",
    "        else:\n",
    "            Status.append(statuscode)\n",
    "    except IndexError:\n",
    "            Status.append(0)\n",
    "            print(\"sa\")\n",
    "            print(i)\n",
    "    try:\n",
    "        rank = soup.findAll(\"rank\")[0].string\n",
    "        if rank is None:\n",
    "            Alexa_rank.append(0)\n",
    "        else:\n",
    "            Alexa_rank.append(rank)\n",
    "    except IndexError:\n",
    "            Alexa_rank.append(0)\n",
    "            print(\"wa\")\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2 = {'URL':modified_url[:154897],'Alexa_rank':Alexa_rank,'Status_code':Status}\n",
    "sample2 = pd.DataFrame(dict2)\n",
    "sample2.to_csv('temp.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "Alexa_rank2 = []\n",
    "Status2 = []\n",
    "for i in tqdm(modified_url[154897:]):\n",
    "    url1 = 'https://awis.api.alexa.com/api?Action=UrlInfo&Count=10&ResponseGroup=Rank,LinksInCount&Start=1&Url='\n",
    "    url = url1+i\n",
    "    headers={'x-api-key':'R90lWSm4iC6L6zDUZnZgs8UmqmtUCFrB6fCT2EY5'}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    try:\n",
    "        statuscode = soup.findAll(\"statuscode\")[0].string\n",
    "        if statuscode is None:\n",
    "            Status2.append(0)\n",
    "        else:\n",
    "            Status2.append(statuscode)\n",
    "    except IndexError:\n",
    "            Status2.append(0)\n",
    "            print(\"sa\")\n",
    "            print(i)\n",
    "    try:\n",
    "        rank = soup.findAll(\"rank\")[0].string\n",
    "        if rank is None:\n",
    "            Alexa_rank2.append(0)\n",
    "        else:\n",
    "            Alexa_rank2.append(rank)\n",
    "    except IndexError:\n",
    "            Alexa_rank2.append(0)\n",
    "            print(\"wa\")\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "Alexa_rank3 = []\n",
    "Status3 = []\n",
    "for i in tqdm(modified_url[154897+27714:]):\n",
    "    url1 = 'https://awis.api.alexa.com/api?Action=UrlInfo&Count=10&ResponseGroup=Rank,LinksInCount&Start=1&Url='\n",
    "    url = url1+i\n",
    "    headers={'x-api-key':'R90lWSm4iC6L6zDUZnZgs8UmqmtUCFrB6fCT2EY5'}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    try:\n",
    "        statuscode = soup.findAll(\"statuscode\")[0].string\n",
    "        if statuscode is None:\n",
    "            Status3.append(0)\n",
    "        else:\n",
    "            Status3.append(statuscode)\n",
    "    except IndexError:\n",
    "            Status3.append(0)\n",
    "            print(\"sa\")\n",
    "            print(i)\n",
    "    try:\n",
    "        rank = soup.findAll(\"rank\")[0].string\n",
    "        if rank is None:\n",
    "            Alexa_rank3.append(0)\n",
    "        else:\n",
    "            Alexa_rank3.append(rank)\n",
    "    except IndexError:\n",
    "            Alexa_rank3.append(0)\n",
    "            print(\"wa\")\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "Alexa_rank4 = []\n",
    "Status4 = []\n",
    "for i in tqdm(modified_url[154897+27714+19284:]):\n",
    "    url1 = 'https://awis.api.alexa.com/api?Action=UrlInfo&Count=10&ResponseGroup=Rank,LinksInCount&Start=1&Url='\n",
    "    url = url1+i\n",
    "    headers={'x-api-key':'R90lWSm4iC6L6zDUZnZgs8UmqmtUCFrB6fCT2EY5'}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    try:\n",
    "        statuscode = soup.findAll(\"statuscode\")[0].string\n",
    "        if statuscode is None:\n",
    "            Status4.append(0)\n",
    "        else:\n",
    "            Status4.append(statuscode)\n",
    "    except IndexError:\n",
    "            Status4.append(0)\n",
    "            print(\"sa\")\n",
    "            print(i)\n",
    "    try:\n",
    "        rank = soup.findAll(\"rank\")[0].string\n",
    "        if rank is None:\n",
    "            Alexa_rank4.append(0)\n",
    "        else:\n",
    "            Alexa_rank4.append(rank)\n",
    "    except IndexError:\n",
    "            Alexa_rank4.append(0)\n",
    "            print(\"wa\")\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alexa_rank_part2 = Alexa_rank2 + Alexa_rank3 + Alexa_rank4\n",
    "Status_part2 = Status2 + Status3 + Status4\n",
    "print(len(Alexa_rank_part2))\n",
    "print(len(Status_part2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alexa_Status_part1 = pd.read_csv('temp.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alexa_Status_part1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alexa_rank_part1 = list(Alexa_Status_part1['Alexa_rank'])\n",
    "Status_part1 = list(Alexa_Status_part1['Status_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alexa_rank_final = Alexa_rank_part1 + Alexa_rank_part2\n",
    "Status_part_final = Status_part1 + Status_part2\n",
    "print(len(Alexa_rank_final))\n",
    "print(len(Status_part_final))\n",
    "print(len(modified_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_edit_dis_with_maclious =  Data_afterAPI['levenshtein_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_final = {'Unified_url':modified_url,'Reachable_URL':Data_afterAPI['Reachable_URL'],'Time_stamp_if_exist':Data_afterAPI['Time_stamp_if_exist'],'Way_back_archived':Data_afterAPI['Way_back_archived'],\"Freenom_top_level_domain\":Freenom_top_level_domain,\"Previous_malicious_top_level_domain_TLD\":Previous_malicious_top_level_domain_TLD,\"Name_length\":Name_length,\"Wrong_spell_List\":Wrong_spell_List,\"word_dic\":word_dic,\"Special_mark\":Special_mark,\"sub_domain\":sub_domain,\"Contain_IP_Adress\":Contain_IP_Adress,\"levenshtein_distance\":levenshtein_edit_dis_with_maclious,\"Alexa_rank\":Alexa_rank_final,\"Status_code\":Status_part_final}\n",
    "sample_data = pd.DataFrame(dict_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Status_code_type =sample_data.groupby(by=['Status_code'])\n",
    "Status_code_type.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data['Status_code'] = sample_data['Status_code'].astype(int)\n",
    "Status_code_type =sample_data.groupby(by=['Status_code'])\n",
    "Status_code_type.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = sample_data.drop(sample_data[sample_data[\"Status_code\"]==0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Status_code_type =sample_data.groupby(by=['Status_code'])\n",
    "Status_code_type.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.to_csv('data_final_version1.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_data = pd.read_csv('data_final_version1.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_all = []\n",
    "for i in sample_data['Time_stamp_if_exist']:\n",
    "    if int(i/10000000000) == 2020:\n",
    "        time_all.append(2)\n",
    "    elif int(i/10000000000) < 2020 and int(i/10000000000) !=0:\n",
    "        time_all.append(1)\n",
    "    else:\n",
    "        time_all.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data['Time_stamp_if_exist'] = time_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89622"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alexa_all = []\n",
    "for i in sample_data['Alexa_rank']:\n",
    "    if i != 0:\n",
    "        Alexa_all.append(1)\n",
    "    else:\n",
    "        Alexa_all.append(0)\n",
    "Alexa_all.count(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data['Alexa_rank'] = Alexa_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unified_url</th>\n",
       "      <th>Reachable_URL</th>\n",
       "      <th>Time_stamp_if_exist</th>\n",
       "      <th>Way_back_archived</th>\n",
       "      <th>Freenom_top_level_domain</th>\n",
       "      <th>Previous_malicious_top_level_domain_TLD</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Wrong_spell_List</th>\n",
       "      <th>word_dic</th>\n",
       "      <th>Special_mark</th>\n",
       "      <th>sub_domain</th>\n",
       "      <th>Contain_IP_Adress</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>Alexa_rank</th>\n",
       "      <th>Status_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://coronavirusemploymentservices.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>26.799492</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://coronavirusen.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://coronavirusencasa.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18.532995</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://coronavirusencolombia.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20.697970</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://coronavirusend.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16.845178</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Unified_url  Reachable_URL  \\\n",
       "0  http://coronavirusemploymentservices.com              0   \n",
       "1                  http://coronavirusen.com              0   \n",
       "2              http://coronavirusencasa.com              0   \n",
       "3          http://coronavirusencolombia.com              0   \n",
       "4                 http://coronavirusend.com              0   \n",
       "\n",
       "   Time_stamp_if_exist  Way_back_archived  Freenom_top_level_domain  \\\n",
       "0                    0                  0                         0   \n",
       "1                    0                  0                         0   \n",
       "2                    0                  0                         0   \n",
       "3                    0                  0                         0   \n",
       "4                    0                  0                         0   \n",
       "\n",
       "   Previous_malicious_top_level_domain_TLD  Name_length  Wrong_spell_List  \\\n",
       "0                                        0            6                 0   \n",
       "1                                        0            5                 0   \n",
       "2                                        0            6                 0   \n",
       "3                                        0            6                 0   \n",
       "4                                        0            5                 0   \n",
       "\n",
       "   word_dic  Special_mark  sub_domain  Contain_IP_Adress  \\\n",
       "0  0.050000             0           2                  0   \n",
       "1  0.083333             0           2                  0   \n",
       "2  0.071429             0           2                  0   \n",
       "3  0.062500             0           2                  0   \n",
       "4  0.080000             0           2                  0   \n",
       "\n",
       "   levenshtein_distance  Alexa_rank  Status_code  \n",
       "0             26.799492           0          200  \n",
       "1             16.500000           0          200  \n",
       "2             18.532995           0          200  \n",
       "3             20.697970           1          200  \n",
       "4             16.845178           0          200  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sample_data.drop(columns = ['Unified_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data.drop(columns = ['First_DBSCAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reachable_URL</th>\n",
       "      <th>Time_stamp_if_exist</th>\n",
       "      <th>Way_back_archived</th>\n",
       "      <th>Freenom_top_level_domain</th>\n",
       "      <th>Previous_malicious_top_level_domain_TLD</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Wrong_spell_List</th>\n",
       "      <th>word_dic</th>\n",
       "      <th>Special_mark</th>\n",
       "      <th>sub_domain</th>\n",
       "      <th>Contain_IP_Adress</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>Alexa_rank</th>\n",
       "      <th>Status_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>26.799492</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18.532995</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20.697970</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16.845178</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Reachable_URL  Time_stamp_if_exist  Way_back_archived  \\\n",
       "0              0                    0                  0   \n",
       "1              0                    0                  0   \n",
       "2              0                    0                  0   \n",
       "3              0                    0                  0   \n",
       "4              0                    0                  0   \n",
       "\n",
       "   Freenom_top_level_domain  Previous_malicious_top_level_domain_TLD  \\\n",
       "0                         0                                        0   \n",
       "1                         0                                        0   \n",
       "2                         0                                        0   \n",
       "3                         0                                        0   \n",
       "4                         0                                        0   \n",
       "\n",
       "   Name_length  Wrong_spell_List  word_dic  Special_mark  sub_domain  \\\n",
       "0            6                 0  0.050000             0           2   \n",
       "1            5                 0  0.083333             0           2   \n",
       "2            6                 0  0.071429             0           2   \n",
       "3            6                 0  0.062500             0           2   \n",
       "4            5                 0  0.080000             0           2   \n",
       "\n",
       "   Contain_IP_Adress  levenshtein_distance  Alexa_rank  Status_code  \n",
       "0                  0             26.799492           0          200  \n",
       "1                  0             16.500000           0          200  \n",
       "2                  0             18.532995           0          200  \n",
       "3                  0             20.697970           1          200  \n",
       "4                  0             16.845178           0          200  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(eps=1.5, min_samples=30)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "clustering = DBSCAN(eps=1.5, min_samples=30, \n",
    "                    metric='euclidean', metric_params=None, algorithm='auto', \n",
    "                    leaf_size=30, p=None, n_jobs=None)\n",
    "clustering.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 18\n",
      "Estimated number of noise points: 1788\n",
      "Silhouette Coefficient: 0.063\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "core_samples_mask = np.zeros_like(clustering.labels_, dtype=bool)\n",
    "core_samples_mask[clustering.core_sample_indices_] = True\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(train_data, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, -1}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(labels)\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['First_DBSCAN'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "First_DBSCAN\n",
       "-1       1788\n",
       " 0     213945\n",
       " 1         36\n",
       " 2         34\n",
       " 3        141\n",
       " 4        916\n",
       " 5       6732\n",
       " 6         34\n",
       " 7         38\n",
       " 8         75\n",
       " 9       2093\n",
       " 10        47\n",
       " 11        57\n",
       " 12        33\n",
       " 13        63\n",
       " 14        30\n",
       " 15        30\n",
       " 16        30\n",
       " 17      3367\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcscan = train_data.groupby(by=['First_DBSCAN'])\n",
    "dcscan.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(624/(228855+624),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/394 [00:00<00:51,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The produced word ratio list's length is  394\n",
      "The sample data is [0.10526315789473684, 0.08695652173913043, 0.10526315789473684, 0.08, 0.046511627906976744, 0.07692307692307693]\n",
      "The number of domains cotain symbol '-': 128\n",
      "The number of domain contain IP adress is 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 394/394 [00:56<00:00,  7.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "all_maclious = confirmed_phishing_website \n",
    "Freenom_top_level_domain_maclious = Extract_FTLD(all_maclious)\n",
    "Previous_malicious_top_level_domain_TLD_maclious = Previous_MTLD(all_maclious)\n",
    "Name_length_maclious, Wrong_spell_List_maclious,word_dic_maclious = mixed_feature(all_maclious)\n",
    "Special_mark_maclious = Find_Dash_mark(all_maclious)\n",
    "sub_domain_maclious = Count_subdomain(all_maclious)\n",
    "Contain_IP_Adress_maclious = Contain_IP_address(all_maclious)\n",
    "levenshtein_edit_dis_with_maclious_maclious = []\n",
    "for i in tqdm(all_maclious):\n",
    "    dis_temp = []\n",
    "    for j in confirmed_phishing_website:\n",
    "        dis = levenshtein(i, j)\n",
    "        dis_temp.append(dis)\n",
    "        distance = np.mean(dis_temp)\n",
    "    levenshtein_edit_dis_with_maclious_maclious.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 394/394 [02:10<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "Alexa_rank5 = []\n",
    "Status5 = []\n",
    "for i in tqdm(all_maclious):\n",
    "    url1 = 'https://awis.api.alexa.com/api?Action=UrlInfo&Count=10&ResponseGroup=Rank,LinksInCount&Start=1&Url='\n",
    "    url = url1+i\n",
    "    headers={'x-api-key':'R90lWSm4iC6L6zDUZnZgs8UmqmtUCFrB6fCT2EY5'}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    try:\n",
    "        statuscode = soup.findAll(\"statuscode\")[0].string\n",
    "        if statuscode is None:\n",
    "            Status5.append(0)\n",
    "        else:\n",
    "            Status5.append(statuscode)\n",
    "    except IndexError:\n",
    "            Status5.append(0)\n",
    "            print(\"sa\")\n",
    "            print(i)\n",
    "    try:\n",
    "        rank = soup.findAll(\"rank\")[0].string\n",
    "        if rank is None:\n",
    "            Alexa_rank5.append(0)\n",
    "        else:\n",
    "            Alexa_rank5.append(rank)\n",
    "    except IndexError:\n",
    "            Alexa_rank5.append(0)\n",
    "            print(\"wa\")\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 394/394 [05:08<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "way_back = []\n",
    "time_stamp = []\n",
    "reachable_url = []\n",
    "for i in tqdm(all_maclious):\n",
    "    try:\n",
    "        url1 = 'https://archive.org/wayback/available?url='\n",
    "        url = url1 + i\n",
    "        response = requests.get(url,timeout=2)\n",
    "        json_data = json.loads(response.text)\n",
    "        status= response.status_code\n",
    "        if status == 200:\n",
    "            reachable_url.append(1)\n",
    "        else:\n",
    "            reachable_url.append(0)\n",
    "        if bool(json_data['archived_snapshots']) is False:\n",
    "            way_back.append(0)\n",
    "            time_stamp.append(0)\n",
    "        else:\n",
    "            way_back.append(1)\n",
    "            time_stamp.append(json_data['archived_snapshots']['closest']['timestamp'])\n",
    "            \n",
    "    except requests.Timeout as err:\n",
    "        way_back.append(0)\n",
    "        time_stamp.append(0)\n",
    "        reachable_url.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394\n",
      "394\n",
      "394\n"
     ]
    }
   ],
   "source": [
    "print(len(way_back))\n",
    "print(len(time_stamp))\n",
    "print(len(reachable_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_maclious = {'Unified_url':all_maclious,'Reachable_URL':reachable_url,'Time_stamp_if_exist':time_stamp,'Way_back_archived':way_back,\"Freenom_top_level_domain\":Freenom_top_level_domain_maclious,\"Previous_malicious_top_level_domain_TLD\":Previous_malicious_top_level_domain_TLD_maclious,\"Name_length\":Name_length_maclious,\"Wrong_spell_List\":Wrong_spell_List_maclious,\"word_dic\":word_dic_maclious,\"Special_mark\":Special_mark_maclious,\"sub_domain\":sub_domain_maclious,\"Contain_IP_Adress\":Contain_IP_Adress_maclious,\"levenshtein_distance\":levenshtein_edit_dis_with_maclious_maclious,\"Alexa_rank\":Alexa_rank5,\"Status_code\":Status5}\n",
    "sample_data_maclious = pd.DataFrame(dict_maclious)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_maclious.to_csv('data_maclious_final.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_maclious = []\n",
    "for i in sample_data_maclious['Time_stamp_if_exist']:\n",
    "    i = int(i)\n",
    "    if int(i/10000000000) == 2020:\n",
    "        time_maclious.append(2)\n",
    "    elif int(i/10000000000) < 2020 and int(i/10000000000) !=0:\n",
    "        time_maclious.append(1)\n",
    "    else:\n",
    "        time_maclious.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394\n"
     ]
    }
   ],
   "source": [
    "print(len(time_maclious))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_maclious['Time_stamp_if_exist'] = time_maclious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alexa_all_maclious = []\n",
    "for i in sample_data_maclious['Alexa_rank']:\n",
    "    if i != 0:\n",
    "        Alexa_all_maclious.append(1)\n",
    "    else:\n",
    "        Alexa_all_maclious.append(0)\n",
    "Alexa_all_maclious.count(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_maclious['Alexa_rank'] = Alexa_all_maclious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unified_url</th>\n",
       "      <th>Reachable_URL</th>\n",
       "      <th>Time_stamp_if_exist</th>\n",
       "      <th>Way_back_archived</th>\n",
       "      <th>Freenom_top_level_domain</th>\n",
       "      <th>Previous_malicious_top_level_domain_TLD</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Wrong_spell_List</th>\n",
       "      <th>word_dic</th>\n",
       "      <th>Special_mark</th>\n",
       "      <th>sub_domain</th>\n",
       "      <th>Contain_IP_Adress</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>Alexa_rank</th>\n",
       "      <th>Status_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://158.69.153.253</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19.786802</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://19covid-gouv12.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16.987310</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://3dforcovid.com</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.027919</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://COVID--19-shop.rf.gd</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21.068528</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://actualisatie.updateics-covid19.noez.me</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.796954</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Unified_url  Reachable_URL  \\\n",
       "0                          http://158.69.153.253              1   \n",
       "1                      http://19covid-gouv12.com              1   \n",
       "2                          http://3dforcovid.com              1   \n",
       "3                    http://COVID--19-shop.rf.gd              1   \n",
       "4  http://actualisatie.updateics-covid19.noez.me              1   \n",
       "\n",
       "   Time_stamp_if_exist  Way_back_archived  Freenom_top_level_domain  \\\n",
       "0                    0                  0                         0   \n",
       "1                    0                  0                         0   \n",
       "2                    2                  1                         0   \n",
       "3                    0                  0                         0   \n",
       "4                    0                  0                         0   \n",
       "\n",
       "   Previous_malicious_top_level_domain_TLD  Name_length  Wrong_spell_List  \\\n",
       "0                                        0            5                 0   \n",
       "1                                        0            8                 0   \n",
       "2                                        0            7                 0   \n",
       "3                                        0            7                 0   \n",
       "4                                        0           12                 0   \n",
       "\n",
       "   word_dic  Special_mark  sub_domain  Contain_IP_Adress  \\\n",
       "0  0.105263             0           3                  1   \n",
       "1  0.086957             1           1                  0   \n",
       "2  0.105263             0           1                  0   \n",
       "3  0.080000             1           2                  0   \n",
       "4  0.046512             1           3                  0   \n",
       "\n",
       "   levenshtein_distance  Alexa_rank Status_code  \n",
       "0             19.786802           0         200  \n",
       "1             16.987310           0         200  \n",
       "2             15.027919           1         200  \n",
       "3             21.068528           1         200  \n",
       "4             29.796954           0         200  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data_maclious.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "maclious_url = sample_data_maclious['Unified_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_maclious = sample_data_maclious.drop(columns = ['Unified_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_predict_maclious = clustering.fit_predict(sample_data_maclious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0 -1 -1 -1 -1  0 -1 -1 -1 -1  0  0  0  0 -1  0  0  0  0  0  0 -1  0\n",
      "  0  0 -1 -1  0 -1 -1  0  0  0 -1 -1 -1  0  0 -1 -1 -1  0 -1 -1  0  0  0\n",
      "  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1  0 -1\n",
      "  0  0  0 -1 -1  0  0  0  0  0  0  0 -1 -1  0 -1 -1 -1  0  0 -1  0 -1  0\n",
      "  0 -1  0 -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  0 -1 -1  0 -1  0 -1 -1\n",
      "  0 -1 -1  0 -1  0 -1 -1 -1 -1 -1 -1 -1  0 -1  0 -1 -1 -1 -1  0 -1  0  0\n",
      "  0  0  0  0  0  0  0  0  0 -1 -1  0 -1  0  0  0  0  0  0  0  0  0 -1  0\n",
      " -1 -1 -1  0 -1 -1  0  0  0  0  0  0  0  0 -1  0 -1  0  0  0 -1  0 -1  0\n",
      " -1  0  0 -1 -1  0  0  0 -1 -1  0 -1 -1  0 -1  0  0 -1  0  0  0  0  0  0\n",
      "  0  0 -1 -1  0  0 -1  0  0  0  0 -1 -1 -1 -1  0  0  0  0  0 -1 -1  0 -1\n",
      "  0  0  0  0  0 -1 -1  0 -1 -1  0  0  0  0  0  0 -1  0 -1 -1  0  0 -1  0\n",
      " -1  0  0 -1 -1 -1 -1  0 -1 -1  0  0  0 -1 -1  0  0 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  0 -1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0  0  0  0  0 -1\n",
      " -1  0  0 -1 -1 -1  0  0  0 -1 -1 -1  0  0 -1 -1  0 -1  0 -1 -1  0  0 -1\n",
      "  0 -1  0  0  0  0 -1  0 -1  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  0  0 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1  0\n",
      " -1  0  0 -1 -1 -1  0  0 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(lable_predict_maclious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_predict_maclious = list(lable_predict_maclious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44416243654822335 %\n"
     ]
    }
   ],
   "source": [
    "print(lable_predict_maclious.count(-1)/(lable_predict_maclious.count(0)+lable_predict_maclious.count(-1)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ure4covid.com\n",
      "http://.contracovid.com.br\n",
      "http://.contracovid.com.br\n",
      "http://ilio.caixa.gov.br\n",
      "http://-coronavirus.ch\n",
      "http://-coronavirus.ch\n",
      "http://ck-covid.org\n",
      "http://panhacoronavirus.barro.ce.gov.br\n",
      "http://s.fmrp.usp.br\n",
      "http://9.oracle.com\n",
      "http://bateaocoronavirus.feira.br\n",
      "http://tracovid.com.br\n",
      "http://ona-data.ch\n",
      "http://ona-stats.co.za\n",
      "http://onamadrid.com\n",
      "http://onavirus.alemparaiba.mg.gov.br\n",
      "http://onavirus.amazonas.am.gov.br\n",
      "http://onavirus.ceara.gov.br\n",
      "http://onavirus.cl\n",
      "http://onavirus.datafree.co\n",
      "http://onavirus.es\n",
      "http://onavirus.es.gov.br\n",
      "http://onavirus.fapce.edu.br\n",
      "http://onavirus.gob.es\n",
      "http://onavirus.informa.poli.ufrj.br\n",
      "http://onavirus.itaborai.rj.gov.br\n",
      "http://onavirus.jhu.edu\n",
      "http://onavirus.ms.gov.br\n",
      "http://onavirus.navirai.ms.gov.br\n",
      "http://onavirus.pr.gov.br\n",
      "http://onavirus.pucgoias.edu.br\n",
      "http://onavirus.rj.gov.br\n",
      "http://onavirus.ro.gov.br\n",
      "http://onavirus.saude.gov.br\n",
      "http://onavirus.sc.gov.br\n",
      "http://onavirus.to.gov.br\n",
      "http://id-19-1-newhavenct.hub.arcgis.com\n",
      "http://id-19.es\n",
      "http://id-19.gob.es\n",
      "http://id-19.iglocska.eu\n",
      "http://id-calc.org\n",
      "http://id-misp.ncsc.gov.ie\n",
      "http://id.apollo247.com\n",
      "http://id.saude.gov.br\n",
      "http://id19-brazil-api.now.sh\n",
      "http://id19-phwstatement.nhs.wales\n",
      "http://id19.criciuma.sc.gov.br\n",
      "http://id19.doisvizinhos.pr.gov.br\n",
      "http://id19.es\n",
      "http://id19.figshare.com\n",
      "http://id19.gob.es\n",
      "http://id19.govt.nz\n",
      "http://id19.ifnoar.com.br\n",
      "http://id19.lapig.iesa.ufg.br\n",
      "http://id19.nhp.gov.in\n",
      "http://id19.oracle.com\n",
      "http://id19.unimedsudoeste.com.br\n",
      "http://id19healthbot.cdc.gov\n",
      "http://id19india.org\n",
      "http://id19japan.com\n",
      "http://id19nasfavelas.meurio.org.br\n",
      "http://id19sa.org\n",
      "http://idcuritiba.com.br\n",
      "http://ovid19.ie\n",
      "http://h-coronavirus.ch\n",
      "http://.ie\n",
      "http://covid19form.com\n",
      "http://covidresearch.com\n",
      "http://pabot-covid19.ai\n",
      "http://pabot-covid19.app\n",
      "http://pabot-covid19.es\n",
      "http://pabot-covid19.eu\n",
      "http://pabot-covid19.gob.es\n",
      "http://pabot-covid19.net\n",
      "http://pabot-covid19.org\n",
      "http://pabotcovid19.ai\n",
      "http://pabotcovid19.app\n",
      "http://pabotcovid19.es\n",
      "http://pabotcovid19.eu\n",
      "http://pabotcovid19.gob.es\n",
      "http://pabotcovid19.net\n",
      "http://pabotcovid19.org\n",
      "http://covid19.ie\n",
      "http://o-coronavirus.be\n",
      "http://toscontracoronavirus.com.br\n",
      "http://v2019.live\n",
      "http://ocoronavirus.recife.pe.gov.br\n",
      "http://p-coronavirus.ch\n",
      "http://ontracoronavirus.pe.gov.br\n",
      "http://oronavirus.co.za\n",
      "http://lucascoronavirus.com.br\n",
      "http://anocoronavirus.com.br\n",
      "http://p-coronavirus.ch\n",
      "http://medcontracoronavirus.com.br\n",
      "http://sourascontraocorona.com.br\n",
      "http://uscorona.cl\n",
      "http://covid.sicredi.com.br\n",
      "http://.contracovid.com.br\n"
     ]
    }
   ],
   "source": [
    "confirmed_white_website = []\n",
    "f = open(\"covid19-related-master/whitelist-domains.txt\", \"r\")\n",
    "for x in f:\n",
    "    print('http://'+ x[3:-1])\n",
    "    confirmed_white_website.append('http://'+ x[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The produced word ratio list's length is  98\n",
      "The sample data is [0.10526315789473684, 0.08, 0.08, 0.08695652173913043, 0.09523809523809523, 0.09523809523809523]\n",
      "The number of domains cotain symbol '-': 24\n",
      "The number of domain contain IP adress is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 98/98 [00:13<00:00,  7.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "all_white = confirmed_white_website \n",
    "Freenom_top_level_domain_white = Extract_FTLD(all_white)\n",
    "Previous_malicious_top_level_domain_TLD_white = Previous_MTLD(all_white)\n",
    "Name_length_white, Wrong_spell_List_white,word_dic_white = mixed_feature(all_white)\n",
    "Special_mark_white = Find_Dash_mark(all_white)\n",
    "sub_domain_white = Count_subdomain(all_white)\n",
    "Contain_IP_Adress_white = Contain_IP_address(all_white)\n",
    "levenshtein_edit_dis_with_maclious_white = []\n",
    "for i in tqdm(all_white):\n",
    "    dis_temp = []\n",
    "    for j in confirmed_phishing_website:\n",
    "        dis = levenshtein(i, j)\n",
    "        dis_temp.append(dis)\n",
    "        distance = np.mean(dis_temp)\n",
    "    levenshtein_edit_dis_with_maclious_white.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 98/98 [00:34<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "Alexa_rank6 = []\n",
    "Status6 = []\n",
    "for i in tqdm(all_white):\n",
    "    url1 = 'https://awis.api.alexa.com/api?Action=UrlInfo&Count=10&ResponseGroup=Rank,LinksInCount&Start=1&Url='\n",
    "    url = url1+i\n",
    "    headers={'x-api-key':'R90lWSm4iC6L6zDUZnZgs8UmqmtUCFrB6fCT2EY5'}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    try:\n",
    "        statuscode = soup.findAll(\"statuscode\")[0].string\n",
    "        if statuscode is None:\n",
    "            Status6.append(0)\n",
    "        else:\n",
    "            Status6.append(statuscode)\n",
    "    except IndexError:\n",
    "            Status6.append(0)\n",
    "            print(\"sa\")\n",
    "            print(i)\n",
    "    try:\n",
    "        rank = soup.findAll(\"rank\")[0].string\n",
    "        if rank is None:\n",
    "            Alexa_rank6.append(0)\n",
    "        else:\n",
    "            Alexa_rank6.append(rank)\n",
    "    except IndexError:\n",
    "            Alexa_rank6.append(0)\n",
    "            print(\"wa\")\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 98/98 [01:44<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "way_back_white = []\n",
    "time_stamp_white = []\n",
    "reachable_url_white = []\n",
    "for i in tqdm(all_white):\n",
    "    try:\n",
    "        url1 = 'https://archive.org/wayback/available?url='\n",
    "        url = url1 + i\n",
    "        response = requests.get(url,timeout=2)\n",
    "        json_data = json.loads(response.text)\n",
    "        status= response.status_code\n",
    "        if status == 200:\n",
    "            reachable_url_white.append(1)\n",
    "        else:\n",
    "            reachable_url_white.append(0)\n",
    "        if bool(json_data['archived_snapshots']) is False:\n",
    "            way_back_white.append(0)\n",
    "            time_stamp_white.append(0)\n",
    "        else:\n",
    "            way_back_white.append(1)\n",
    "            time_stamp_white.append(json_data['archived_snapshots']['closest']['timestamp'])\n",
    "            \n",
    "    except requests.Timeout as err:\n",
    "        way_back_white.append(0)\n",
    "        time_stamp_white.append(0)\n",
    "        reachable_url_white.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_white = {'Unified_url':all_white,'Reachable_URL':reachable_url_white,'Time_stamp_if_exist':time_stamp_white,'Way_back_archived':way_back_white,\"Freenom_top_level_domain\":Freenom_top_level_domain_white,\"Previous_malicious_top_level_domain_TLD\":Previous_malicious_top_level_domain_TLD_white,\"Name_length\":Name_length_white,\"Wrong_spell_List\":Wrong_spell_List_white,\"word_dic\":word_dic_white,\"Special_mark\":Special_mark_white,\"sub_domain\":sub_domain_white,\"Contain_IP_Adress\":Contain_IP_Adress_white,\"levenshtein_distance\":levenshtein_edit_dis_with_maclious_white,\"Alexa_rank\":Alexa_rank6,\"Status_code\":Status6}\n",
    "sample_data_white = pd.DataFrame(dict_white)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_white.to_csv('data_white_final.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_white = []\n",
    "for i in sample_data_white['Time_stamp_if_exist']:\n",
    "    i = int(i)\n",
    "    if int(i/10000000000) == 2020:\n",
    "        time_white.append(2)\n",
    "    elif int(i/10000000000) < 2020 and int(i/10000000000) !=0:\n",
    "        time_white.append(1)\n",
    "    else:\n",
    "        time_white.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_white['Time_stamp_if_exist'] = time_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alexa_all_white = []\n",
    "for i in sample_data_white['Alexa_rank']:\n",
    "    if i != 0:\n",
    "        Alexa_all_white.append(1)\n",
    "    else:\n",
    "        Alexa_all_white.append(0)\n",
    "Alexa_all_white.count(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_white['Alexa_rank'] = Alexa_all_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unified_url</th>\n",
       "      <th>Reachable_URL</th>\n",
       "      <th>Time_stamp_if_exist</th>\n",
       "      <th>Way_back_archived</th>\n",
       "      <th>Freenom_top_level_domain</th>\n",
       "      <th>Previous_malicious_top_level_domain_TLD</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Wrong_spell_List</th>\n",
       "      <th>word_dic</th>\n",
       "      <th>Special_mark</th>\n",
       "      <th>sub_domain</th>\n",
       "      <th>Contain_IP_Adress</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>Alexa_rank</th>\n",
       "      <th>Status_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://ure4covid.com\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.436548</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://.contracovid.com.br\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>17.642132</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://.contracovid.com.br\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>17.642132</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://ilio.caixa.gov.br\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18.807107</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://-coronavirus.ch\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16.482234</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Unified_url  Reachable_URL  Time_stamp_if_exist  \\\n",
       "0        http://ure4covid.com\\n              1                    0   \n",
       "1  http://.contracovid.com.br\\n              1                    0   \n",
       "2  http://.contracovid.com.br\\n              1                    0   \n",
       "3    http://ilio.caixa.gov.br\\n              1                    0   \n",
       "4      http://-coronavirus.ch\\n              1                    0   \n",
       "\n",
       "   Way_back_archived  Freenom_top_level_domain  \\\n",
       "0                  0                         0   \n",
       "1                  0                         0   \n",
       "2                  0                         0   \n",
       "3                  0                         0   \n",
       "4                  0                         0   \n",
       "\n",
       "   Previous_malicious_top_level_domain_TLD  Name_length  Wrong_spell_List  \\\n",
       "0                                        0            6                 0   \n",
       "1                                        0            6                 0   \n",
       "2                                        0            6                 0   \n",
       "3                                        0            8                 0   \n",
       "4                                        0            3                 0   \n",
       "\n",
       "   word_dic  Special_mark  sub_domain  Contain_IP_Adress  \\\n",
       "0  0.105263             0           1                  0   \n",
       "1  0.080000             0           3                  0   \n",
       "2  0.080000             0           3                  0   \n",
       "3  0.086957             0           3                  0   \n",
       "4  0.095238             1           1                  0   \n",
       "\n",
       "   levenshtein_distance  Alexa_rank Status_code  \n",
       "0             15.436548           0         200  \n",
       "1             17.642132           0         200  \n",
       "2             17.642132           0         200  \n",
       "3             18.807107           0         200  \n",
       "4             16.482234           0         200  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data_white.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_white = sample_data_white.drop(columns=['Unified_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_predict_white = clustering.fit_predict(sample_data_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(lable_predict_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 %\n"
     ]
    }
   ],
   "source": [
    "lable_predict_white = list(lable_predict_white)\n",
    "print(lable_predict_white.count(-1)/(lable_predict_white.count(0)+lable_predict_white.count(-1)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
